{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Net_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruy-tRiWv6Vm",
        "outputId": "d5e6461d-2b33-4fc5-b7ce-46c122986e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install -q xlrd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I. Data Preprocessing**"
      ],
      "metadata": {
        "id": "a5VLntxY4HKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1. This part Play the Responsibility: preprocess the y value and numpy array X of data by Construct several function corresponding to each data type.***"
      ],
      "metadata": {
        "id": "xNWI5hjNxcb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data expected to the pandas dataframe\n",
        "df1 = pd.read_csv('/content/drive/My Drive/Data/EPA_merged_2019-2021.csv').drop(columns = ['co', 'no2', 'pm10'])\n",
        "\n",
        "# This function resolve the problem of unsynchronized data format in df1 'time' landmark\n",
        "def timeConvert(stringTime):\n",
        "  if not stringTime.startswith(\"2021\") and not stringTime.startswith(\"2020\"):\n",
        "    return datetime.datetime.strptime(stringTime, '%d/%m/%Y %H:%M')\n",
        "  return datetime.datetime.strptime(stringTime, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "df1['time'] = df1['time'].apply(lambda x : timeConvert(x))\n",
        "df1['Year'] = pd.DatetimeIndex(df1['time']).year\n",
        "df1['Month'] = pd.DatetimeIndex(df1['time']).month\n",
        "# Try calculating average of weight groupby Month. But it's necessary to determine that it's not just groupby Month column, it also depend on Year .\n",
        "df1['Year-Month'] = df1.apply(lambda x: '%s-%s' % (x['Year'], x['Month']), axis = 1)\n",
        "Month_Series_AVE = df1.groupby(['name', 'Year-Month']).mean()"
      ],
      "metadata": {
        "id": "s4aS2aCQxKD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Month_Series_AVE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAnkK4pVKRSA",
        "outputId": "77ab9326-1690-4d8f-cce3-a7ad6327bd4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         pm25      lat       lon  Year  Month\n",
            "name    Year-Month                                           \n",
            "ankhanh 2020-10      8.356183  21.0024  105.7181  2020     10\n",
            "        2020-11     12.632778  21.0024  105.7181  2020     11\n",
            "        2020-12     26.127688  21.0024  105.7181  2020     12\n",
            "        2020-5      21.984010  21.0024  105.7181  2020      5\n",
            "        2020-6      12.413769  21.0024  105.7181  2020      6\n",
            "...                       ...      ...       ...   ...    ...\n",
            "xuanmai 2021-2      55.598653  20.8994  105.5773  2021      2\n",
            "        2021-3      50.795380  20.8994  105.5773  2021      3\n",
            "        2021-4      40.802364  20.8994  105.5773  2021      4\n",
            "        2021-5      31.248925  20.8994  105.5773  2021      5\n",
            "        2021-6      39.545263  20.8994  105.5773  2021      6\n",
            "\n",
            "[620 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data in full packet without split to train, validation.\n",
        "Xfull = []\n",
        "Yfull = []"
      ],
      "metadata": {
        "id": "3kSfIOvuce4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from osgeo import gdal\n",
        "from osgeo import ogr\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Ba9a7-qV_vqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "dR7yoGapI5o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyproj\n",
        "from pyproj import Proj\n",
        "prj = Proj('+proj=utm +zone=48 +ellps=WGS84 +datum=WGS84 +units=m +no_defs', preserve_units=False, errcheck=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNWbpdNiVIwX",
        "outputId": "bc33ca57-30e9-4d3a-c947-87d5b2e5ff0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyproj\n",
            "  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from pyproj) (2021.10.8)\n",
            "Installing collected packages: pyproj\n",
            "Successfully installed pyproj-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function get the value around point corresponding to particular datatype eg: population, land cover, traffic tile, ...\n",
        "def getValueAroundPoint(img_data, geotransform, lat, lon, width, nodata, dtype):\n",
        "    result = np.array((), dtype=np.float32)   \n",
        "    offset_x = (lat - geotransform[3])/geotransform[5]\n",
        "    offset_y = (lon - geotransform[0])/geotransform[1]\n",
        "    evenWidth = ((width%2)==0)\n",
        "    radius = (width-1)/2.0\n",
        "    maxOffset = int(np.floor(radius))\n",
        "    if evenWidth:\n",
        "        maxOffset = maxOffset + 1\n",
        "        center_offset_x = np.ceil(offset_x)\n",
        "        center_offset_y = np.floor(offset_y)\n",
        "    elif not evenWidth:\n",
        "        center_offset_x = int(np.floor(offset_x) + 0.5)\n",
        "        center_offset_y = int(np.floor(offset_y) + 0.5)\n",
        "    # print(center_offset_x)\n",
        "    # print(center_offset_y)\n",
        "    minOffset = int(np.floor(radius) * -1)\n",
        "    for y in range(minOffset, maxOffset + 1):\n",
        "        for x in range(minOffset, maxOffset + 1):\n",
        "            double_x = float(x)\n",
        "            double_y = float(y)\n",
        "            real_x = int(center_offset_x - double_x)\n",
        "            real_y = int(center_offset_y + double_y)\n",
        "            if evenWidth:\n",
        "                double_x = double_x - 0.5 \n",
        "                double_y = double_y - 0.5  \n",
        "                real_x = int(center_offset_x - (double_x + 0.5))\n",
        "                real_y = int(center_offset_y + (double_y + 0.5))\n",
        "            distance = np.sqrt((double_x * double_x) + (double_y * double_y))\n",
        "            if (0 < real_x < img_data.shape[0]) and (0 < real_y < img_data.shape[1]):\n",
        "                if (distance <= radius):\n",
        "                    if (img_data[real_x][real_y] != nodata):\n",
        "                        if (not np.isnan(img_data[real_x][real_y])):\n",
        "                            result = np.append(result, img_data[real_x][real_y])\n",
        "    # print(result.size)\n",
        "    if (result.size > 0):\n",
        "        if dtype == 'pop' or dtype == 'traffic' or dtype == 'distribution':\n",
        "            return np.array([result.mean()], dtype=np.float32)\n",
        "        elif dtype == 'lc':\n",
        "            count_1 = np.count_nonzero(result == 1)\n",
        "            count_2 = np.count_nonzero(result == 2)\n",
        "            count_3 = np.count_nonzero(result == 3)\n",
        "            count_4 = np.count_nonzero(result == 4)\n",
        "            count_5 = np.count_nonzero(result == 5)\n",
        "            count_6 = np.count_nonzero(result == 6)\n",
        "            count_7 = np.count_nonzero(result == 7)\n",
        "            return np.array([count_1, count_2, count_3, count_4, count_5, count_6, count_7], dtype=np.float32)\n",
        "    else:\n",
        "        if dtype == 'pop' or dtype == 'traffic' or dtype == 'distribution':\n",
        "            return np.array([0], dtype=np.float32)\n",
        "        elif dtype == 'lc':\n",
        "            return np.array([0, 0, 0, 0, 0, 0, 0], dtype=np.float32)"
      ],
      "metadata": {
        "id": "rLJ21hKNLYaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get traffic tile in practical map.\n",
        "def getTrafficTile(lat, lon, width):\n",
        "  path1 = \"/content/drive/My Drive/Data/average_average_30m.tif\"\n",
        "  path2 = \"/content/drive/My Drive/Data/average_normal_30m.tif\"\n",
        "  path3 = \"/content/drive/My Drive/Data/average_rush_30m.tif\"\n",
        "  ds1 = gdal.Open(path1)\n",
        "  ds1_band = ds1.GetRasterBand(1)\n",
        "  nodataval1 = ds1_band.GetNoDataValue()\n",
        "  gt1 = ds1.GetGeoTransform()\n",
        "  ds1_arr = ds1.ReadAsArray().astype(np.float64)\n",
        "\n",
        "  ds2 = gdal.Open(path2)\n",
        "  ds2_band = ds2.GetRasterBand(1)\n",
        "  nodataval2 = ds2_band.GetNoDataValue()\n",
        "  gt2 = ds2.GetGeoTransform()\n",
        "  ds2_arr = ds2.ReadAsArray().astype(np.float64)\n",
        "\n",
        "  ds3 = gdal.Open(path3)\n",
        "  ds3_band = ds3.GetRasterBand(1)\n",
        "  nodataval3 = ds3_band.GetNoDataValue()\n",
        "  gt3 = ds3.GetGeoTransform()\n",
        "  ds3_arr = ds3.ReadAsArray().astype(np.float64)\n",
        "  avg = getValueAroundPoint(ds1_arr, gt1, prj(lon, lat)[1], prj(lon, lat)[0], width/gt1[1], nodataval1, 'traffic')\n",
        "  normal = getValueAroundPoint(ds2_arr, gt2, prj(lon, lat)[1], prj(lon, lat)[0], width/gt2[1], nodataval2, 'traffic')\n",
        "  rush = getValueAroundPoint(ds3_arr, gt3, prj(lon, lat)[1], prj(lon, lat)[0], width/gt3[1], nodataval3, 'traffic')\n",
        "  return np.array([avg, normal, rush])"
      ],
      "metadata": {
        "id": "rZoVEmm6iR6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get raw data PM25 in practical map.\n",
        "def getRawPM25(lat, lon, year, month, width):\n",
        "  path = \"/content/drive/My Drive/Data/\" + str(year) + \"/PM25_\" + str(year)\n",
        "\n",
        "  if month < 10: path = path + '0' + str(month) + '01_hanoi.tif'\n",
        "  else: path = path + str(month) + '01_hanoi.tif'\n",
        "\n",
        "  ds = gdal.Open(path)\n",
        "  ds_band = ds.GetRasterBand(1)\n",
        "  nodataval = ds_band.GetNoDataValue()\n",
        "  ds_arr = ds.ReadAsArray().astype(np.float64)\n",
        "  gt = ds.GetGeoTransform()\n",
        "  return getValueAroundPoint(ds_arr, gt, prj(lon, lat)[1], prj(lon, lat)[0], width/gt[1], nodataval, 'distribution')"
      ],
      "metadata": {
        "id": "YM5jJ8Z2_iXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Population density through year statistical.\n",
        "def getPopDensity(lat, lon, year, width):\n",
        "  path = \"/content/drive/My Drive/Data/hanoi_pd_\" + str(year) + \"_30m_Unadj.tif\"\n",
        "  ds = gdal.Open(path)\n",
        "  ds_band = ds.GetRasterBand(1)\n",
        "  nodataval = ds_band.GetNoDataValue()\n",
        "  ds_arr = ds.ReadAsArray().astype(np.float64)\n",
        "  gt = ds.GetGeoTransform()\n",
        "  return getValueAroundPoint(ds_arr, gt, prj(lon, lat)[1], prj(lon, lat)[0], width/gt[1], nodataval, 'pop')"
      ],
      "metadata": {
        "id": "HsoxS9_Nr2K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Land Cover in Common\n",
        "def getLandCover(lat, lon, width):\n",
        "  path = \"/content/drive/My Drive/Data/lc_2017.tif\"\n",
        "  ds = gdal.Open(path)\n",
        "  ds_band = ds.GetRasterBand(1)\n",
        "  nodataval = ds_band.GetNoDataValue()\n",
        "  ds_arr = ds.ReadAsArray().astype(np.float64)\n",
        "  gt = ds.GetGeoTransform()\n",
        "  return getValueAroundPoint(ds_arr, gt, prj(lon, lat)[1], prj(lon, lat)[0], width/gt[1], nodataval, 'lc')"
      ],
      "metadata": {
        "id": "R0X353t4uA4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get road density statistical\n",
        "def getRoadDensity(lat, lon, width):\n",
        "  path = \"/content/drive/My Drive/Data/road_density_HN_30m_Clipped.tif\"\n",
        "  ds = gdal.Open(path)\n",
        "  ds_band = ds.GetRasterBand(1)\n",
        "  nodataval = ds_band.GetNoDataValue()\n",
        "  ds_arr = ds.ReadAsArray().astype(np.float64)\n",
        "  gt = ds.GetGeoTransform()\n",
        "  return getValueAroundPoint(ds_arr, gt, prj(lon, lat)[1], prj(lon, lat)[0], width/gt[1], nodataval, 'traffic')"
      ],
      "metadata": {
        "id": "vWA1V8npwFlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get VVNB feature\n",
        "df2 = pd.read_csv('/content/drive/My Drive/Data/VVNB_2019-2021_mean_by_month.csv')\n",
        "def getVVNB(year, month):\n",
        "  strquery = \"year == '\" + str(year) + \"' and month == '\" + str(year) + \"-\"\n",
        "  if month < 10: strquery = strquery + \"0\" + str(month) + \"'\"\n",
        "  else: strquery = strquery + str(month) + \"'\"\n",
        "  altm = df2.query(strquery).iloc[0, 2]\n",
        "  temp = df2.query(strquery).iloc[0, 3]\n",
        "  hud = df2.query(strquery).iloc[0, 4]\n",
        "  wdir = df2.query(strquery).iloc[0, 5]\n",
        "  wspd = df2.query(strquery).iloc[0, 6]\n",
        "  vis = df2.query(strquery).iloc[0, 7]\n",
        "  return np.array([altm, temp, hud, wdir, wspd, vis])"
      ],
      "metadata": {
        "id": "-ks_8YyD5E8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2. This part is beginning extract data to store into numpy array***"
      ],
      "metadata": {
        "id": "E8yY6ETseEba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Big Step to store data\n",
        "# Constantly Width Hyper Parameter: \n",
        "width = 1000\n",
        "for i in range(0, len(Month_Series_AVE['lat'])):\n",
        "  if Month_Series_AVE.iloc[i, 3] == 2021: continue\n",
        "  # One datapoint\n",
        "  # label had been added first\n",
        "  Yfull.append(np.array([round(Month_Series_AVE.iloc[i, 0], 4)]))\n",
        "  lat = round(Month_Series_AVE.iloc[i, 1], 4)\n",
        "  lon = round(Month_Series_AVE.iloc[i, 2], 4)\n",
        "  year = Month_Series_AVE.iloc[i, 3]\n",
        "  month = Month_Series_AVE.iloc[i, 4]\n",
        "  # inject rawPm25\n",
        "  X = getRawPM25(lat, lon, year, month, width).reshape(1, -1)\n",
        "  # inject traffic tile\n",
        "  X = np.concatenate((X, getTrafficTile(lat, lon, width).reshape(1, -1)), axis = 1)\n",
        "  # inject land cover\n",
        "  X = np.concatenate((X, getLandCover(lat, lon, width).reshape(1, -1)), axis = 1)\n",
        "  # inject road density\n",
        "  X = np.concatenate((X, getRoadDensity(lat, lon, width).reshape(1, -1)), axis = 1)\n",
        "  # inject population density\n",
        "  X = np.concatenate((X, getPopDensity(lat, lon, year, width).reshape(1, -1)), axis = 1)\n",
        "  # inject VVNB feature\n",
        "  X = np.concatenate((X, getVVNB(year, month).reshape(1, -1)), axis = 1)\n",
        "  Xfull.append(X.reshape(19))"
      ],
      "metadata": {
        "id": "VpKMgiQO0jj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **II. Building the Model**"
      ],
      "metadata": {
        "id": "rtF3aukgUkYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "# tensorboard evaluate support\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "metadata": {
        "id": "rqt8b-nO5U1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15291891-e8ac-4fe6-9582-d1f1d36eaaf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f0ef297c150>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "L5Jm2qs6Y4kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "EQRWHdfE71El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import math"
      ],
      "metadata": {
        "id": "SttOAey779fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1. Prepare dataset and call it to tensor.***"
      ],
      "metadata": {
        "id": "x-z-xhrLXJ9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_x = torch.Tensor(Xfull)\n",
        "tensor_y = torch.Tensor(Yfull)\n",
        "tensor_x.unsqueeze(1)\n",
        "tensor_x.unsqueeze(1)\n",
        "# normalize input\n",
        "tensor_x_mean = torch.mean(tensor_x)\n",
        "tensor_x_std = torch.std(tensor_x)\n",
        "tensor_x = tensor_x - tensor_x_mean\n",
        "tensor_x = tensor_x / tensor_x_std\n",
        "\n",
        "# normalize target\n",
        "tensor_y_mean = torch.mean(tensor_y)\n",
        "tensor_y_std = torch.std(tensor_y)\n",
        "tensor_y_norm = tensor_y - tensor_y_mean\n",
        "tensor_y_norm = tensor_y_norm / tensor_y_std\n",
        "\n",
        "# tensor_y = tensor_y.view((len(Yfull), 1, 1))\n",
        "my_dataset = TensorDataset(tensor_x, tensor_y_norm)\n",
        "# my_dataloader = DataLoader(my_dataset, batch_size = 8)\n",
        "# Split datasets into 25% validation and 75% train with Hyper parameter batch size = 8.\n",
        "train_idx, val_idx = train_test_split(list(range(len(my_dataset))), test_size=0.25)\n",
        "datasets = {}\n",
        "datasets['train'] = Subset(my_dataset, train_idx)\n",
        "datasets['validate'] = Subset(my_dataset, val_idx)\n",
        "dataloaders = {x:DataLoader(datasets[x], 4, shuffle=False, num_workers= 1) for x in ['train','validate']}"
      ],
      "metadata": {
        "id": "e5a0mceEWo0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2. Construct the Neural Network Model with 2 hiden layer and 1 output in last layer***"
      ],
      "metadata": {
        "id": "285KBidxedZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Defining The Loss function Mean Square Error, Optimizer update model parameter using Stochastic Gradient Descent with Backpropagation.*"
      ],
      "metadata": {
        "id": "myIpD1RqetbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pl.log_every_n_steps = 2"
      ],
      "metadata": {
        "id": "v7-1bHnFAy5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(pl.LightningModule):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(19, 1950, bias = True),\n",
        "      nn.LeakyReLU(),\n",
        "      nn.Linear(1950, 950, bias = True),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(950, 1, bias = True),\n",
        "    )\n",
        "    self.ce = torch.nn.MSELoss()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "  \n",
        "  def training_step(self, batch, batch_idx):\n",
        "    x, y = batch\n",
        "    x = x.view(x.size(0), -1)\n",
        "    y_hat = self.layers(x)\n",
        "    # print(y_hat)\n",
        "    # print(y)\n",
        "    loss = self.ce(y_hat, y)\n",
        "    self.log('train_loss', loss)\n",
        "    return loss\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum = 0.9)\n",
        "    return optimizer\n",
        "\n",
        "model = NeuralNet()\n",
        "# As Long Using Stochastic Gradient Descent to optimizing model params\n",
        "# Hyper Parameter learning rate = 0.001 and momentum = 0.9 to pass the local Extreme point. Need to statistical in more time training.\n",
        "# Using Mean Square Error, aka L2norm distance.\n",
        "training_loader = torch.utils.data.DataLoader(datasets['train'], batch_size=4, shuffle=True, num_workers=1, pin_memory=True)\n",
        "validation_loader = torch.utils.data.DataLoader(datasets['validate'], batch_size=4, shuffle=True, num_workers=1, pin_memory=True)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum = 0.9)"
      ],
      "metadata": {
        "id": "Sg3wQ2DTVsk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pl.seed_everything(42)\n",
        "trainer = pl.Trainer(gpus=0, deterministic=True, max_epochs=20)\n",
        "trainer.fit(model, training_loader)"
      ],
      "metadata": {
        "id": "miFpzpjj82VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3. Start Training and Evaluating Model***"
      ],
      "metadata": {
        "id": "Rgyb9wnefW_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Step by Step What the forward and backward affected the model parameter*"
      ],
      "metadata": {
        "id": "Y2grZ1sugM1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(epoch_index, tb_writer):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    for i, data in enumerate(training_loader):\n",
        "        inputs, target = data\n",
        "        \n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "        # Make predictions for this batch\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, target)\n",
        "        # Backpropagation\n",
        "        loss.backward(retain_graph=True)\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        last_loss = running_loss/4  # loss per batch\n",
        "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "        tb_x = epoch_index * len(training_loader) + i + 1\n",
        "        tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
        "        running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "4DYatm7ngLdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Training and Evaluate in 20 epochs*"
      ],
      "metadata": {
        "id": "sjo6-HKwkL8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "epoch_number = 0\n",
        "\n",
        "EPOCHS = 30\n",
        "\n",
        "best_vloss = 1_000_000.\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train(True)\n",
        "    avg_loss = train_one_epoch(epoch_number, writer)\n",
        "\n",
        "    # We don't need gradients on to do reporting\n",
        "    model.train(False)\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    for i, vdata in enumerate(validation_loader):\n",
        "        vinputs, vlabels = vdata\n",
        "        voutputs = model(vinputs)\n",
        "        vloss = loss_fn(voutputs, vlabels)\n",
        "        running_vloss += vloss\n",
        "\n",
        "    avg_vloss = running_vloss / (i + 1)\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # Log the running loss averaged per batch\n",
        "    # for both training and validation\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                    epoch_number + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    # Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss:\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cExj0SH0fdUs",
        "outputId": "a9e47661-4191-47fd-d724-701ab087e074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "  batch 1 loss: 0.5040348172187805\n",
            "  batch 2 loss: 0.20588332414627075\n",
            "  batch 3 loss: 0.1448315531015396\n",
            "  batch 4 loss: 0.1543264389038086\n",
            "  batch 5 loss: 0.32019567489624023\n",
            "  batch 6 loss: 0.6655056476593018\n",
            "  batch 7 loss: 0.20674821734428406\n",
            "  batch 8 loss: 0.23021455109119415\n",
            "  batch 9 loss: 0.28883984684944153\n",
            "  batch 10 loss: 0.5924987196922302\n",
            "  batch 11 loss: 0.5358014702796936\n",
            "  batch 12 loss: 0.24778775870800018\n",
            "  batch 13 loss: 0.17660938203334808\n",
            "  batch 14 loss: 0.1408720314502716\n",
            "  batch 15 loss: 0.4146944582462311\n",
            "  batch 16 loss: 0.24226036667823792\n",
            "  batch 17 loss: 0.38123422861099243\n",
            "  batch 18 loss: 0.14072054624557495\n",
            "  batch 19 loss: 0.3346032500267029\n",
            "  batch 20 loss: 0.17846998572349548\n",
            "  batch 21 loss: 0.25450974702835083\n",
            "  batch 22 loss: 0.06074845418334007\n",
            "  batch 23 loss: 0.17443467676639557\n",
            "  batch 24 loss: 0.24581021070480347\n",
            "  batch 25 loss: 0.6123690009117126\n",
            "  batch 26 loss: 0.4004790484905243\n",
            "  batch 27 loss: 0.5497849583625793\n",
            "  batch 28 loss: 0.12828129529953003\n",
            "  batch 29 loss: 0.3826429843902588\n",
            "  batch 30 loss: 0.08026942610740662\n",
            "  batch 31 loss: 0.04330606758594513\n",
            "  batch 32 loss: 0.35369622707366943\n",
            "  batch 33 loss: 0.16918694972991943\n",
            "  batch 34 loss: 0.18616291880607605\n",
            "  batch 35 loss: 0.4596141278743744\n",
            "  batch 36 loss: 0.0511074922978878\n",
            "  batch 37 loss: 0.09176981449127197\n",
            "  batch 38 loss: 0.11695194244384766\n",
            "  batch 39 loss: 0.2833123505115509\n",
            "  batch 40 loss: 0.21747055649757385\n",
            "  batch 41 loss: 0.20081129670143127\n",
            "  batch 42 loss: 0.04685879498720169\n",
            "  batch 43 loss: 0.258583128452301\n",
            "  batch 44 loss: 0.3264565169811249\n",
            "  batch 45 loss: 0.12946361303329468\n",
            "  batch 46 loss: 0.29490935802459717\n",
            "  batch 47 loss: 0.11442141234874725\n",
            "  batch 48 loss: 0.10574141144752502\n",
            "  batch 49 loss: 0.3660389184951782\n",
            "  batch 50 loss: 0.2440982162952423\n",
            "  batch 51 loss: 0.09267821907997131\n",
            "  batch 52 loss: 0.19333428144454956\n",
            "  batch 53 loss: 0.10491172969341278\n",
            "  batch 54 loss: 0.07788477092981339\n",
            "  batch 55 loss: 0.16791197657585144\n",
            "  batch 56 loss: 0.17161957919597626\n",
            "  batch 57 loss: 0.16696172952651978\n",
            "  batch 58 loss: 0.14246493577957153\n",
            "  batch 59 loss: 0.2137618064880371\n",
            "  batch 60 loss: 0.29715853929519653\n",
            "  batch 61 loss: 0.14524459838867188\n",
            "  batch 62 loss: 0.31278669834136963\n",
            "  batch 63 loss: 0.11152400076389313\n",
            "  batch 64 loss: 0.23381540179252625\n",
            "  batch 65 loss: 0.107852041721344\n",
            "  batch 66 loss: 0.29245778918266296\n",
            "  batch 67 loss: 0.2200600951910019\n",
            "  batch 68 loss: 0.3534989655017853\n",
            "  batch 69 loss: 0.8149744272232056\n",
            "  batch 70 loss: 0.4407309293746948\n",
            "  batch 71 loss: 0.17027196288108826\n",
            "  batch 72 loss: 0.20514892041683197\n",
            "  batch 73 loss: 0.09399116039276123\n",
            "  batch 74 loss: 0.1498810350894928\n",
            "  batch 75 loss: 0.40163469314575195\n",
            "  batch 76 loss: 0.20228710770606995\n",
            "  batch 77 loss: 0.07915877550840378\n",
            "  batch 78 loss: 0.25315192341804504\n",
            "  batch 79 loss: 0.11138791590929031\n",
            "  batch 80 loss: 0.12928637862205505\n",
            "LOSS train 0.12928637862205505 valid 1.0299415588378906\n",
            "EPOCH 2:\n",
            "  batch 1 loss: 0.12024971842765808\n",
            "  batch 2 loss: 0.05079925060272217\n",
            "  batch 3 loss: 0.6132705211639404\n",
            "  batch 4 loss: 0.16173282265663147\n",
            "  batch 5 loss: 0.07431075721979141\n",
            "  batch 6 loss: 0.15429562330245972\n",
            "  batch 7 loss: 0.2756580710411072\n",
            "  batch 8 loss: 0.179347425699234\n",
            "  batch 9 loss: 0.08279265463352203\n",
            "  batch 10 loss: 0.18793168663978577\n",
            "  batch 11 loss: 0.1788722723722458\n",
            "  batch 12 loss: 0.30127784609794617\n",
            "  batch 13 loss: 0.15539616346359253\n",
            "  batch 14 loss: 0.060202233493328094\n",
            "  batch 15 loss: 0.6306559443473816\n",
            "  batch 16 loss: 0.101918525993824\n",
            "  batch 17 loss: 0.24795220792293549\n",
            "  batch 18 loss: 0.4831187129020691\n",
            "  batch 19 loss: 0.15843886137008667\n",
            "  batch 20 loss: 0.21271532773971558\n",
            "  batch 21 loss: 0.18148766458034515\n",
            "  batch 22 loss: 0.6489904522895813\n",
            "  batch 23 loss: 0.1862214058637619\n",
            "  batch 24 loss: 0.09219571202993393\n",
            "  batch 25 loss: 0.17332854866981506\n",
            "  batch 26 loss: 0.1251567006111145\n",
            "  batch 27 loss: 0.09936226904392242\n",
            "  batch 28 loss: 0.16443532705307007\n",
            "  batch 29 loss: 0.10349255800247192\n",
            "  batch 30 loss: 0.2591487169265747\n",
            "  batch 31 loss: 0.06269386410713196\n",
            "  batch 32 loss: 0.09951252490282059\n",
            "  batch 33 loss: 0.15775078535079956\n",
            "  batch 34 loss: 0.15485160052776337\n",
            "  batch 35 loss: 0.16547542810440063\n",
            "  batch 36 loss: 0.21447348594665527\n",
            "  batch 37 loss: 0.11398836225271225\n",
            "  batch 38 loss: 0.1826339215040207\n",
            "  batch 39 loss: 0.35766375064849854\n",
            "  batch 40 loss: 0.12685637176036835\n",
            "  batch 41 loss: 0.252526193857193\n",
            "  batch 42 loss: 0.34138911962509155\n",
            "  batch 43 loss: 0.17557573318481445\n",
            "  batch 44 loss: 0.15071389079093933\n",
            "  batch 45 loss: 0.7290922403335571\n",
            "  batch 46 loss: 0.40924474596977234\n",
            "  batch 47 loss: 0.6106549501419067\n",
            "  batch 48 loss: 0.261905699968338\n",
            "  batch 49 loss: 0.21635328233242035\n",
            "  batch 50 loss: 0.24193905293941498\n",
            "  batch 51 loss: 0.16596585512161255\n",
            "  batch 52 loss: 0.09346622228622437\n",
            "  batch 53 loss: 0.2634776830673218\n",
            "  batch 54 loss: 0.28628063201904297\n",
            "  batch 55 loss: 0.4617882966995239\n",
            "  batch 56 loss: 0.3276919424533844\n",
            "  batch 57 loss: 0.2763664722442627\n",
            "  batch 58 loss: 0.21547538042068481\n",
            "  batch 59 loss: 0.16251659393310547\n",
            "  batch 60 loss: 0.10687252879142761\n",
            "  batch 61 loss: 0.1802523285150528\n",
            "  batch 62 loss: 0.13995233178138733\n",
            "  batch 63 loss: 0.20729482173919678\n",
            "  batch 64 loss: 0.2365296632051468\n",
            "  batch 65 loss: 0.41927608847618103\n",
            "  batch 66 loss: 0.19644097983837128\n",
            "  batch 67 loss: 0.12138809263706207\n",
            "  batch 68 loss: 0.3147883713245392\n",
            "  batch 69 loss: 0.12165747582912445\n",
            "  batch 70 loss: 0.1022946760058403\n",
            "  batch 71 loss: 0.19893230497837067\n",
            "  batch 72 loss: 0.11476308107376099\n",
            "  batch 73 loss: 0.15706518292427063\n",
            "  batch 74 loss: 0.04936765134334564\n",
            "  batch 75 loss: 0.45980140566825867\n",
            "  batch 76 loss: 0.18347381055355072\n",
            "  batch 77 loss: 0.2563657760620117\n",
            "  batch 78 loss: 0.5724438428878784\n",
            "  batch 79 loss: 0.1311006397008896\n",
            "  batch 80 loss: 0.0951390266418457\n",
            "LOSS train 0.0951390266418457 valid 1.0693391561508179\n",
            "EPOCH 3:\n",
            "  batch 1 loss: 0.366811066865921\n",
            "  batch 2 loss: 0.18053562939167023\n",
            "  batch 3 loss: 0.18118426203727722\n",
            "  batch 4 loss: 0.07898877561092377\n",
            "  batch 5 loss: 0.019912593066692352\n",
            "  batch 6 loss: 0.10141398012638092\n",
            "  batch 7 loss: 0.5049536228179932\n",
            "  batch 8 loss: 0.12092579156160355\n",
            "  batch 9 loss: 0.18527783453464508\n",
            "  batch 10 loss: 0.18227186799049377\n",
            "  batch 11 loss: 0.6479740738868713\n",
            "  batch 12 loss: 0.20149558782577515\n",
            "  batch 13 loss: 0.3303167521953583\n",
            "  batch 14 loss: 0.12621532380580902\n",
            "  batch 15 loss: 0.66419917345047\n",
            "  batch 16 loss: 0.18939362466335297\n",
            "  batch 17 loss: 0.2290370762348175\n",
            "  batch 18 loss: 0.24942056834697723\n",
            "  batch 19 loss: 0.18741117417812347\n",
            "  batch 20 loss: 0.08512252569198608\n",
            "  batch 21 loss: 0.11396768689155579\n",
            "  batch 22 loss: 0.23253348469734192\n",
            "  batch 23 loss: 0.07793749868869781\n",
            "  batch 24 loss: 0.2930392622947693\n",
            "  batch 25 loss: 0.3155119717121124\n",
            "  batch 26 loss: 0.14413319528102875\n",
            "  batch 27 loss: 0.5691719055175781\n",
            "  batch 28 loss: 0.3434825837612152\n",
            "  batch 29 loss: 0.3561810553073883\n",
            "  batch 30 loss: 0.06445707380771637\n",
            "  batch 31 loss: 0.060007575899362564\n",
            "  batch 32 loss: 0.18454734981060028\n",
            "  batch 33 loss: 0.10592874884605408\n",
            "  batch 34 loss: 0.17438925802707672\n",
            "  batch 35 loss: 0.12462897598743439\n",
            "  batch 36 loss: 0.21236097812652588\n",
            "  batch 37 loss: 0.21463924646377563\n",
            "  batch 38 loss: 0.5038242936134338\n",
            "  batch 39 loss: 0.09263908863067627\n",
            "  batch 40 loss: 0.07495903223752975\n",
            "  batch 41 loss: 0.12383262813091278\n",
            "  batch 42 loss: 0.21659192442893982\n",
            "  batch 43 loss: 0.165313720703125\n",
            "  batch 44 loss: 0.1234421581029892\n",
            "  batch 45 loss: 0.28501564264297485\n",
            "  batch 46 loss: 0.10537777096033096\n",
            "  batch 47 loss: 0.19417741894721985\n",
            "  batch 48 loss: 0.15083326399326324\n",
            "  batch 49 loss: 0.010281703434884548\n",
            "  batch 50 loss: 0.2769806981086731\n",
            "  batch 51 loss: 0.20322665572166443\n",
            "  batch 52 loss: 0.2306954711675644\n",
            "  batch 53 loss: 0.25132155418395996\n",
            "  batch 54 loss: 0.31271904706954956\n",
            "  batch 55 loss: 0.10815566033124924\n",
            "  batch 56 loss: 0.11816303431987762\n",
            "  batch 57 loss: 0.1280287653207779\n",
            "  batch 58 loss: 0.16815456748008728\n",
            "  batch 59 loss: 0.077384352684021\n",
            "  batch 60 loss: 0.2601761519908905\n",
            "  batch 61 loss: 0.24927543103694916\n",
            "  batch 62 loss: 0.0821436271071434\n",
            "  batch 63 loss: 0.09462891519069672\n",
            "  batch 64 loss: 0.5419459342956543\n",
            "  batch 65 loss: 0.37502923607826233\n",
            "  batch 66 loss: 0.11075656861066818\n",
            "  batch 67 loss: 0.19828155636787415\n",
            "  batch 68 loss: 0.5190103054046631\n",
            "  batch 69 loss: 0.09032054990530014\n",
            "  batch 70 loss: 0.21786053478717804\n",
            "  batch 71 loss: 0.1511513590812683\n",
            "  batch 72 loss: 0.09182614088058472\n",
            "  batch 73 loss: 0.4714488685131073\n",
            "  batch 74 loss: 0.06141547113656998\n",
            "  batch 75 loss: 0.4108401834964752\n",
            "  batch 76 loss: 0.15021851658821106\n",
            "  batch 77 loss: 0.21277117729187012\n",
            "  batch 78 loss: 0.1905827522277832\n",
            "  batch 79 loss: 0.26069629192352295\n",
            "  batch 80 loss: 0.15675106644630432\n",
            "LOSS train 0.15675106644630432 valid 1.0506591796875\n",
            "EPOCH 4:\n",
            "  batch 1 loss: 0.12344616651535034\n",
            "  batch 2 loss: 0.17319552600383759\n",
            "  batch 3 loss: 0.12813790142536163\n",
            "  batch 4 loss: 0.030737753957509995\n",
            "  batch 5 loss: 0.43657082319259644\n",
            "  batch 6 loss: 0.1902788281440735\n",
            "  batch 7 loss: 0.12181733548641205\n",
            "  batch 8 loss: 0.08696269989013672\n",
            "  batch 9 loss: 0.1995641589164734\n",
            "  batch 10 loss: 0.18683896958827972\n",
            "  batch 11 loss: 0.2653096318244934\n",
            "  batch 12 loss: 0.15244778990745544\n",
            "  batch 13 loss: 0.2440272867679596\n",
            "  batch 14 loss: 0.24689221382141113\n",
            "  batch 15 loss: 0.20693336427211761\n",
            "  batch 16 loss: 0.06660745292901993\n",
            "  batch 17 loss: 0.37968945503234863\n",
            "  batch 18 loss: 0.11992835253477097\n",
            "  batch 19 loss: 0.4825454652309418\n",
            "  batch 20 loss: 0.08576499670743942\n",
            "  batch 21 loss: 0.3047560453414917\n",
            "  batch 22 loss: 0.05930554121732712\n",
            "  batch 23 loss: 0.20803041756153107\n",
            "  batch 24 loss: 0.7196770310401917\n",
            "  batch 25 loss: 0.1517140418291092\n",
            "  batch 26 loss: 0.35608020424842834\n",
            "  batch 27 loss: 0.22001683712005615\n",
            "  batch 28 loss: 0.16571718454360962\n",
            "  batch 29 loss: 0.07432502508163452\n",
            "  batch 30 loss: 0.25188323855400085\n",
            "  batch 31 loss: 0.27807167172431946\n",
            "  batch 32 loss: 0.41069191694259644\n",
            "  batch 33 loss: 0.15180020034313202\n",
            "  batch 34 loss: 0.1006956622004509\n",
            "  batch 35 loss: 0.2211119681596756\n",
            "  batch 36 loss: 0.08477535098791122\n",
            "  batch 37 loss: 0.30185410380363464\n",
            "  batch 38 loss: 0.020016631111502647\n",
            "  batch 39 loss: 0.24827410280704498\n",
            "  batch 40 loss: 0.10538829863071442\n",
            "  batch 41 loss: 0.3131099343299866\n",
            "  batch 42 loss: 0.18483863770961761\n",
            "  batch 43 loss: 0.2999112904071808\n",
            "  batch 44 loss: 0.09617942571640015\n",
            "  batch 45 loss: 0.08123517036437988\n",
            "  batch 46 loss: 0.05385541915893555\n",
            "  batch 47 loss: 0.02297990396618843\n",
            "  batch 48 loss: 0.32059532403945923\n",
            "  batch 49 loss: 0.05917510762810707\n",
            "  batch 50 loss: 0.13098718225955963\n",
            "  batch 51 loss: 0.10779324173927307\n",
            "  batch 52 loss: 0.06709395349025726\n",
            "  batch 53 loss: 0.08636079728603363\n",
            "  batch 54 loss: 0.32355403900146484\n",
            "  batch 55 loss: 0.22209392488002777\n",
            "  batch 56 loss: 0.22215883433818817\n",
            "  batch 57 loss: 0.15910707414150238\n",
            "  batch 58 loss: 0.15314728021621704\n",
            "  batch 59 loss: 0.10859046131372452\n",
            "  batch 60 loss: 0.0952330082654953\n",
            "  batch 61 loss: 0.20007124543190002\n",
            "  batch 62 loss: 0.6894265413284302\n",
            "  batch 63 loss: 0.4024970829486847\n",
            "  batch 64 loss: 0.21666912734508514\n",
            "  batch 65 loss: 0.09105858206748962\n",
            "  batch 66 loss: 0.12815839052200317\n",
            "  batch 67 loss: 0.35255587100982666\n",
            "  batch 68 loss: 0.2810623049736023\n",
            "  batch 69 loss: 0.43924447894096375\n",
            "  batch 70 loss: 0.24991586804389954\n",
            "  batch 71 loss: 0.5548163056373596\n",
            "  batch 72 loss: 0.36073943972587585\n",
            "  batch 73 loss: 0.6737326383590698\n",
            "  batch 74 loss: 0.073476642370224\n",
            "  batch 75 loss: 0.2719051241874695\n",
            "  batch 76 loss: 0.07535835355520248\n",
            "  batch 77 loss: 0.21750852465629578\n",
            "  batch 78 loss: 0.0944177433848381\n",
            "  batch 79 loss: 0.13876381516456604\n",
            "  batch 80 loss: 0.3481815755367279\n",
            "LOSS train 0.3481815755367279 valid 1.0749913454055786\n",
            "EPOCH 5:\n",
            "  batch 1 loss: 0.2704230844974518\n",
            "  batch 2 loss: 0.27942049503326416\n",
            "  batch 3 loss: 0.037560056895017624\n",
            "  batch 4 loss: 0.03672340512275696\n",
            "  batch 5 loss: 0.11750169098377228\n",
            "  batch 6 loss: 0.1906910240650177\n",
            "  batch 7 loss: 0.1808629035949707\n",
            "  batch 8 loss: 0.25262272357940674\n",
            "  batch 9 loss: 0.33034631609916687\n",
            "  batch 10 loss: 0.2907090187072754\n",
            "  batch 11 loss: 0.1460999846458435\n",
            "  batch 12 loss: 0.22743180394172668\n",
            "  batch 13 loss: 0.11720253527164459\n",
            "  batch 14 loss: 0.5094658136367798\n",
            "  batch 15 loss: 0.7914859056472778\n",
            "  batch 16 loss: 0.21468140184879303\n",
            "  batch 17 loss: 0.20710475742816925\n",
            "  batch 18 loss: 0.21184219419956207\n",
            "  batch 19 loss: 0.10227016359567642\n",
            "  batch 20 loss: 0.07462049275636673\n",
            "  batch 21 loss: 0.07618115097284317\n",
            "  batch 22 loss: 0.07675730437040329\n",
            "  batch 23 loss: 0.23223119974136353\n",
            "  batch 24 loss: 0.20206521451473236\n",
            "  batch 25 loss: 0.029640011489391327\n",
            "  batch 26 loss: 0.11897770315408707\n",
            "  batch 27 loss: 0.11402107775211334\n",
            "  batch 28 loss: 0.09447380900382996\n",
            "  batch 29 loss: 0.2573527693748474\n",
            "  batch 30 loss: 0.14799673855304718\n",
            "  batch 31 loss: 0.20038165152072906\n",
            "  batch 32 loss: 0.18269583582878113\n",
            "  batch 33 loss: 0.20109331607818604\n",
            "  batch 34 loss: 0.3998671770095825\n",
            "  batch 35 loss: 0.0893680527806282\n",
            "  batch 36 loss: 0.632256805896759\n",
            "  batch 37 loss: 0.32179057598114014\n",
            "  batch 38 loss: 0.07743625342845917\n",
            "  batch 39 loss: 0.1944301426410675\n",
            "  batch 40 loss: 0.17873495817184448\n",
            "  batch 41 loss: 0.15100648999214172\n",
            "  batch 42 loss: 0.07420031726360321\n",
            "  batch 43 loss: 0.31359246373176575\n",
            "  batch 44 loss: 0.294450581073761\n",
            "  batch 45 loss: 0.11866170167922974\n",
            "  batch 46 loss: 0.2294657826423645\n",
            "  batch 47 loss: 0.05252077430486679\n",
            "  batch 48 loss: 0.18540841341018677\n",
            "  batch 49 loss: 0.3316303491592407\n",
            "  batch 50 loss: 0.11887834221124649\n",
            "  batch 51 loss: 0.09663759171962738\n",
            "  batch 52 loss: 0.3282737135887146\n",
            "  batch 53 loss: 0.544580340385437\n",
            "  batch 54 loss: 0.3480536937713623\n",
            "  batch 55 loss: 0.35785195231437683\n",
            "  batch 56 loss: 0.025813449174165726\n",
            "  batch 57 loss: 0.1544736921787262\n",
            "  batch 58 loss: 0.2002185434103012\n",
            "  batch 59 loss: 0.12411748617887497\n",
            "  batch 60 loss: 0.02557462453842163\n",
            "  batch 61 loss: 0.6544526815414429\n",
            "  batch 62 loss: 0.13230296969413757\n",
            "  batch 63 loss: 0.23931896686553955\n",
            "  batch 64 loss: 0.18539918959140778\n",
            "  batch 65 loss: 0.24479787051677704\n",
            "  batch 66 loss: 0.5487469434738159\n",
            "  batch 67 loss: 0.27503612637519836\n",
            "  batch 68 loss: 0.20481902360916138\n",
            "  batch 69 loss: 0.1794130504131317\n",
            "  batch 70 loss: 0.12644392251968384\n",
            "  batch 71 loss: 0.23910239338874817\n",
            "  batch 72 loss: 0.12152886390686035\n",
            "  batch 73 loss: 0.16869527101516724\n",
            "  batch 74 loss: 0.19201518595218658\n",
            "  batch 75 loss: 0.3298627436161041\n",
            "  batch 76 loss: 0.21352604031562805\n",
            "  batch 77 loss: 0.19517278671264648\n",
            "  batch 78 loss: 0.09260980784893036\n",
            "  batch 79 loss: 0.06904911249876022\n",
            "  batch 80 loss: 0.30393093824386597\n",
            "LOSS train 0.30393093824386597 valid 1.1293388605117798\n",
            "EPOCH 6:\n",
            "  batch 1 loss: 0.23044507205486298\n",
            "  batch 2 loss: 0.22141334414482117\n",
            "  batch 3 loss: 0.21825429797172546\n",
            "  batch 4 loss: 0.1934002935886383\n",
            "  batch 5 loss: 0.021124225109815598\n",
            "  batch 6 loss: 0.26518896222114563\n",
            "  batch 7 loss: 0.2478397786617279\n",
            "  batch 8 loss: 0.22970439493656158\n",
            "  batch 9 loss: 0.20423337817192078\n",
            "  batch 10 loss: 0.09498056024312973\n",
            "  batch 11 loss: 0.34164851903915405\n",
            "  batch 12 loss: 0.14725607633590698\n",
            "  batch 13 loss: 0.08377048373222351\n",
            "  batch 14 loss: 0.18964119255542755\n",
            "  batch 15 loss: 0.10280556976795197\n",
            "  batch 16 loss: 0.05486036837100983\n",
            "  batch 17 loss: 0.15582175552845\n",
            "  batch 18 loss: 0.4357500672340393\n",
            "  batch 19 loss: 0.1992187201976776\n",
            "  batch 20 loss: 0.33059948682785034\n",
            "  batch 21 loss: 0.33778733015060425\n",
            "  batch 22 loss: 0.2187427133321762\n",
            "  batch 23 loss: 0.05978909134864807\n",
            "  batch 24 loss: 0.3313443958759308\n",
            "  batch 25 loss: 0.2009008675813675\n",
            "  batch 26 loss: 0.11004512012004852\n",
            "  batch 27 loss: 0.0732511579990387\n",
            "  batch 28 loss: 0.16851821541786194\n",
            "  batch 29 loss: 0.1212325319647789\n",
            "  batch 30 loss: 0.22001978754997253\n",
            "  batch 31 loss: 0.11881907284259796\n",
            "  batch 32 loss: 0.25091034173965454\n",
            "  batch 33 loss: 0.24131926894187927\n",
            "  batch 34 loss: 0.10974763333797455\n",
            "  batch 35 loss: 0.09303000569343567\n",
            "  batch 36 loss: 0.07198639214038849\n",
            "  batch 37 loss: 0.224585622549057\n",
            "  batch 38 loss: 0.24274717271327972\n",
            "  batch 39 loss: 0.16800616681575775\n",
            "  batch 40 loss: 0.3076319694519043\n",
            "  batch 41 loss: 0.05163891613483429\n",
            "  batch 42 loss: 0.10742020606994629\n",
            "  batch 43 loss: 0.061964187771081924\n",
            "  batch 44 loss: 0.31005436182022095\n",
            "  batch 45 loss: 0.1756187379360199\n",
            "  batch 46 loss: 0.1560622602701187\n",
            "  batch 47 loss: 0.250911146402359\n",
            "  batch 48 loss: 0.5136170387268066\n",
            "  batch 49 loss: 0.24097391963005066\n",
            "  batch 50 loss: 0.6983310580253601\n",
            "  batch 51 loss: 0.29871758818626404\n",
            "  batch 52 loss: 0.28244996070861816\n",
            "  batch 53 loss: 0.2131660431623459\n",
            "  batch 54 loss: 0.3030751943588257\n",
            "  batch 55 loss: 0.25497063994407654\n",
            "  batch 56 loss: 0.06499879062175751\n",
            "  batch 57 loss: 0.2752523422241211\n",
            "  batch 58 loss: 0.14256618916988373\n",
            "  batch 59 loss: 0.051578134298324585\n",
            "  batch 60 loss: 0.23513244092464447\n",
            "  batch 61 loss: 0.2442551702260971\n",
            "  batch 62 loss: 0.1905779391527176\n",
            "  batch 63 loss: 0.018125716596841812\n",
            "  batch 64 loss: 0.0841783881187439\n",
            "  batch 65 loss: 0.7480713129043579\n",
            "  batch 66 loss: 0.09854850172996521\n",
            "  batch 67 loss: 0.21468761563301086\n",
            "  batch 68 loss: 0.21819916367530823\n",
            "  batch 69 loss: 0.4419163465499878\n",
            "  batch 70 loss: 0.42693203687667847\n",
            "  batch 71 loss: 0.6317682862281799\n",
            "  batch 72 loss: 0.30562204122543335\n",
            "  batch 73 loss: 0.2741374671459198\n",
            "  batch 74 loss: 0.10712254047393799\n",
            "  batch 75 loss: 0.018807003274559975\n",
            "  batch 76 loss: 0.1388956904411316\n",
            "  batch 77 loss: 0.030021145939826965\n",
            "  batch 78 loss: 0.08849382400512695\n",
            "  batch 79 loss: 0.6795099377632141\n",
            "  batch 80 loss: 0.09510211646556854\n",
            "LOSS train 0.09510211646556854 valid 1.03779935836792\n",
            "EPOCH 7:\n",
            "  batch 1 loss: 0.09149397909641266\n",
            "  batch 2 loss: 0.15214985609054565\n",
            "  batch 3 loss: 0.058899909257888794\n",
            "  batch 4 loss: 0.12148880958557129\n",
            "  batch 5 loss: 0.122034952044487\n",
            "  batch 6 loss: 0.23068974912166595\n",
            "  batch 7 loss: 0.07517687231302261\n",
            "  batch 8 loss: 0.08758964389562607\n",
            "  batch 9 loss: 0.37893199920654297\n",
            "  batch 10 loss: 0.318751722574234\n",
            "  batch 11 loss: 0.0977480411529541\n",
            "  batch 12 loss: 0.03475530445575714\n",
            "  batch 13 loss: 0.17810936272144318\n",
            "  batch 14 loss: 0.08575191348791122\n",
            "  batch 15 loss: 0.12774181365966797\n",
            "  batch 16 loss: 0.44301897287368774\n",
            "  batch 17 loss: 0.13117268681526184\n",
            "  batch 18 loss: 0.34507033228874207\n",
            "  batch 19 loss: 0.14976033568382263\n",
            "  batch 20 loss: 0.2969155013561249\n",
            "  batch 21 loss: 0.1393919289112091\n",
            "  batch 22 loss: 0.22259005904197693\n",
            "  batch 23 loss: 0.14152869582176208\n",
            "  batch 24 loss: 0.8802512288093567\n",
            "  batch 25 loss: 0.14109353721141815\n",
            "  batch 26 loss: 0.09984121471643448\n",
            "  batch 27 loss: 0.3675004839897156\n",
            "  batch 28 loss: 0.0999118760228157\n",
            "  batch 29 loss: 0.5827858448028564\n",
            "  batch 30 loss: 0.34864988923072815\n",
            "  batch 31 loss: 0.23782211542129517\n",
            "  batch 32 loss: 0.3022267818450928\n",
            "  batch 33 loss: 0.12150925397872925\n",
            "  batch 34 loss: 0.16899698972702026\n",
            "  batch 35 loss: 0.7169995307922363\n",
            "  batch 36 loss: 0.34065765142440796\n",
            "  batch 37 loss: 0.2179173231124878\n",
            "  batch 38 loss: 0.05911298841238022\n",
            "  batch 39 loss: 0.02392299845814705\n",
            "  batch 40 loss: 0.15523669123649597\n",
            "  batch 41 loss: 0.2213452309370041\n",
            "  batch 42 loss: 0.5395406484603882\n",
            "  batch 43 loss: 0.12178945541381836\n",
            "  batch 44 loss: 0.38988035917282104\n",
            "  batch 45 loss: 0.053548842668533325\n",
            "  batch 46 loss: 0.40164023637771606\n",
            "  batch 47 loss: 0.13920874893665314\n",
            "  batch 48 loss: 0.1963280886411667\n",
            "  batch 49 loss: 0.3068709671497345\n",
            "  batch 50 loss: 0.1783239245414734\n",
            "  batch 51 loss: 0.12563540041446686\n",
            "  batch 52 loss: 0.25139912962913513\n",
            "  batch 53 loss: 0.1282140612602234\n",
            "  batch 54 loss: 0.10785043239593506\n",
            "  batch 55 loss: 0.17820045351982117\n",
            "  batch 56 loss: 0.3969886302947998\n",
            "  batch 57 loss: 0.10002633184194565\n",
            "  batch 58 loss: 0.3355420231819153\n",
            "  batch 59 loss: 0.18891099095344543\n",
            "  batch 60 loss: 0.10250527411699295\n",
            "  batch 61 loss: 0.15013326704502106\n",
            "  batch 62 loss: 0.3990750014781952\n",
            "  batch 63 loss: 0.149000346660614\n",
            "  batch 64 loss: 0.12464242428541183\n",
            "  batch 65 loss: 0.1444251388311386\n",
            "  batch 66 loss: 0.01587003841996193\n",
            "  batch 67 loss: 0.19675946235656738\n",
            "  batch 68 loss: 0.24336393177509308\n",
            "  batch 69 loss: 0.14856664836406708\n",
            "  batch 70 loss: 0.049379151314496994\n",
            "  batch 71 loss: 0.18710383772850037\n",
            "  batch 72 loss: 0.33877143263816833\n",
            "  batch 73 loss: 0.19715958833694458\n",
            "  batch 74 loss: 0.12887217104434967\n",
            "  batch 75 loss: 0.05993414670228958\n",
            "  batch 76 loss: 0.3266773819923401\n",
            "  batch 77 loss: 0.2590070068836212\n",
            "  batch 78 loss: 0.16297554969787598\n",
            "  batch 79 loss: 0.30485618114471436\n",
            "  batch 80 loss: 0.12481740117073059\n",
            "LOSS train 0.12481740117073059 valid 1.0312806367874146\n",
            "EPOCH 8:\n",
            "  batch 1 loss: 0.16160118579864502\n",
            "  batch 2 loss: 0.24723437428474426\n",
            "  batch 3 loss: 0.2319580763578415\n",
            "  batch 4 loss: 0.2732359766960144\n",
            "  batch 5 loss: 0.12996193766593933\n",
            "  batch 6 loss: 0.07349082827568054\n",
            "  batch 7 loss: 0.1300971508026123\n",
            "  batch 8 loss: 0.012006111443042755\n",
            "  batch 9 loss: 0.30541327595710754\n",
            "  batch 10 loss: 0.05956529825925827\n",
            "  batch 11 loss: 0.5863904356956482\n",
            "  batch 12 loss: 0.06621327996253967\n",
            "  batch 13 loss: 0.1155531108379364\n",
            "  batch 14 loss: 0.33772796392440796\n",
            "  batch 15 loss: 0.1268991231918335\n",
            "  batch 16 loss: 0.08030413091182709\n",
            "  batch 17 loss: 0.5137392282485962\n",
            "  batch 18 loss: 0.25601980090141296\n",
            "  batch 19 loss: 0.16550612449645996\n",
            "  batch 20 loss: 0.21865633130073547\n",
            "  batch 21 loss: 0.24579782783985138\n",
            "  batch 22 loss: 0.15978626906871796\n",
            "  batch 23 loss: 0.03544545918703079\n",
            "  batch 24 loss: 0.09395787119865417\n",
            "  batch 25 loss: 0.2501431405544281\n",
            "  batch 26 loss: 0.06306537240743637\n",
            "  batch 27 loss: 0.060193128883838654\n",
            "  batch 28 loss: 0.06638455390930176\n",
            "  batch 29 loss: 0.39053627848625183\n",
            "  batch 30 loss: 0.19158820807933807\n",
            "  batch 31 loss: 0.5249344706535339\n",
            "  batch 32 loss: 0.2452692687511444\n",
            "  batch 33 loss: 0.11856945604085922\n",
            "  batch 34 loss: 0.13981011509895325\n",
            "  batch 35 loss: 0.18355168402194977\n",
            "  batch 36 loss: 0.16722102463245392\n",
            "  batch 37 loss: 0.3865031599998474\n",
            "  batch 38 loss: 0.1022379994392395\n",
            "  batch 39 loss: 0.02426939085125923\n",
            "  batch 40 loss: 0.1741764098405838\n",
            "  batch 41 loss: 0.16301992535591125\n",
            "  batch 42 loss: 0.03625660762190819\n",
            "  batch 43 loss: 0.15218152105808258\n",
            "  batch 44 loss: 0.16530930995941162\n",
            "  batch 45 loss: 0.15539339184761047\n",
            "  batch 46 loss: 0.08180979639291763\n",
            "  batch 47 loss: 0.0636361837387085\n",
            "  batch 48 loss: 0.3585158586502075\n",
            "  batch 49 loss: 0.24793626368045807\n",
            "  batch 50 loss: 0.1752476841211319\n",
            "  batch 51 loss: 0.05558418110013008\n",
            "  batch 52 loss: 0.19483190774917603\n",
            "  batch 53 loss: 0.19402861595153809\n",
            "  batch 54 loss: 0.1559617817401886\n",
            "  batch 55 loss: 0.9221312999725342\n",
            "  batch 56 loss: 0.12236626446247101\n",
            "  batch 57 loss: 0.5840083360671997\n",
            "  batch 58 loss: 0.19734126329421997\n",
            "  batch 59 loss: 0.09481723606586456\n",
            "  batch 60 loss: 0.2495409995317459\n",
            "  batch 61 loss: 0.47567635774612427\n",
            "  batch 62 loss: 0.2812651991844177\n",
            "  batch 63 loss: 0.24564921855926514\n",
            "  batch 64 loss: 0.24472561478614807\n",
            "  batch 65 loss: 0.1837567836046219\n",
            "  batch 66 loss: 0.1830824315547943\n",
            "  batch 67 loss: 0.08005642145872116\n",
            "  batch 68 loss: 0.41810178756713867\n",
            "  batch 69 loss: 0.36535653471946716\n",
            "  batch 70 loss: 0.4151357114315033\n",
            "  batch 71 loss: 0.06494920700788498\n",
            "  batch 72 loss: 0.2228696644306183\n",
            "  batch 73 loss: 0.06439924240112305\n",
            "  batch 74 loss: 0.2913079559803009\n",
            "  batch 75 loss: 0.39190053939819336\n",
            "  batch 76 loss: 0.11985860764980316\n",
            "  batch 77 loss: 0.1558055877685547\n",
            "  batch 78 loss: 0.20896176993846893\n",
            "  batch 79 loss: 0.149234339594841\n",
            "  batch 80 loss: 0.6355041861534119\n",
            "LOSS train 0.6355041861534119 valid 0.9968124628067017\n",
            "EPOCH 9:\n",
            "  batch 1 loss: 0.19364717602729797\n",
            "  batch 2 loss: 0.1290949583053589\n",
            "  batch 3 loss: 0.34680116176605225\n",
            "  batch 4 loss: 0.03341654688119888\n",
            "  batch 5 loss: 0.18615210056304932\n",
            "  batch 6 loss: 0.235830157995224\n",
            "  batch 7 loss: 0.14285190403461456\n",
            "  batch 8 loss: 0.07225894182920456\n",
            "  batch 9 loss: 0.11572291702032089\n",
            "  batch 10 loss: 0.18725797533988953\n",
            "  batch 11 loss: 0.1112000122666359\n",
            "  batch 12 loss: 0.1934831440448761\n",
            "  batch 13 loss: 0.12408871203660965\n",
            "  batch 14 loss: 0.33124926686286926\n",
            "  batch 15 loss: 0.34292179346084595\n",
            "  batch 16 loss: 0.4175916910171509\n",
            "  batch 17 loss: 0.139809712767601\n",
            "  batch 18 loss: 0.3370250165462494\n",
            "  batch 19 loss: 0.07307387888431549\n",
            "  batch 20 loss: 0.15939196944236755\n",
            "  batch 21 loss: 0.24417157471179962\n",
            "  batch 22 loss: 0.28458723425865173\n",
            "  batch 23 loss: 0.11672770231962204\n",
            "  batch 24 loss: 0.2089722454547882\n",
            "  batch 25 loss: 0.12979835271835327\n",
            "  batch 26 loss: 0.20449894666671753\n",
            "  batch 27 loss: 0.04594842344522476\n",
            "  batch 28 loss: 0.17075471580028534\n",
            "  batch 29 loss: 0.31943655014038086\n",
            "  batch 30 loss: 0.14678677916526794\n",
            "  batch 31 loss: 0.09439997375011444\n",
            "  batch 32 loss: 0.0666041225194931\n",
            "  batch 33 loss: 0.4155426621437073\n",
            "  batch 34 loss: 0.31972262263298035\n",
            "  batch 35 loss: 0.2619591951370239\n",
            "  batch 36 loss: 0.15739715099334717\n",
            "  batch 37 loss: 0.050422847270965576\n",
            "  batch 38 loss: 0.11558380722999573\n",
            "  batch 39 loss: 0.09643219411373138\n",
            "  batch 40 loss: 0.2994454503059387\n",
            "  batch 41 loss: 0.20586439967155457\n",
            "  batch 42 loss: 0.10597148537635803\n",
            "  batch 43 loss: 0.09898293018341064\n",
            "  batch 44 loss: 0.07838793098926544\n",
            "  batch 45 loss: 0.2161564826965332\n",
            "  batch 46 loss: 0.16423474252223969\n",
            "  batch 47 loss: 0.13224297761917114\n",
            "  batch 48 loss: 0.2660261392593384\n",
            "  batch 49 loss: 0.8489606380462646\n",
            "  batch 50 loss: 0.03514930605888367\n",
            "  batch 51 loss: 0.8059571385383606\n",
            "  batch 52 loss: 0.28991684317588806\n",
            "  batch 53 loss: 0.21408787369728088\n",
            "  batch 54 loss: 0.29902273416519165\n",
            "  batch 55 loss: 0.13592670857906342\n",
            "  batch 56 loss: 0.09019306302070618\n",
            "  batch 57 loss: 0.12121729552745819\n",
            "  batch 58 loss: 0.11293865740299225\n",
            "  batch 59 loss: 0.42885512113571167\n",
            "  batch 60 loss: 0.12735970318317413\n",
            "  batch 61 loss: 0.6373633146286011\n",
            "  batch 62 loss: 0.1854069083929062\n",
            "  batch 63 loss: 0.07927992194890976\n",
            "  batch 64 loss: 0.13174286484718323\n",
            "  batch 65 loss: 0.15672455728054047\n",
            "  batch 66 loss: 0.29120883345603943\n",
            "  batch 67 loss: 0.08981825411319733\n",
            "  batch 68 loss: 0.17978698015213013\n",
            "  batch 69 loss: 0.20759490132331848\n",
            "  batch 70 loss: 0.3114238977432251\n",
            "  batch 71 loss: 0.182926207780838\n",
            "  batch 72 loss: 0.4400419294834137\n",
            "  batch 73 loss: 0.633320689201355\n",
            "  batch 74 loss: 0.14577758312225342\n",
            "  batch 75 loss: 0.0873960480093956\n",
            "  batch 76 loss: 0.14434319734573364\n",
            "  batch 77 loss: 0.20516827702522278\n",
            "  batch 78 loss: 0.13625875115394592\n",
            "  batch 79 loss: 0.13515739142894745\n",
            "  batch 80 loss: 0.32871565222740173\n",
            "LOSS train 0.32871565222740173 valid 0.981707751750946\n",
            "EPOCH 10:\n",
            "  batch 1 loss: 0.4020303189754486\n",
            "  batch 2 loss: 0.2240801751613617\n",
            "  batch 3 loss: 0.20107799768447876\n",
            "  batch 4 loss: 0.08217170089483261\n",
            "  batch 5 loss: 0.1004268079996109\n",
            "  batch 6 loss: 0.10929019749164581\n",
            "  batch 7 loss: 0.11064107716083527\n",
            "  batch 8 loss: 0.2625284492969513\n",
            "  batch 9 loss: 0.13558748364448547\n",
            "  batch 10 loss: 0.2713533639907837\n",
            "  batch 11 loss: 0.31050312519073486\n",
            "  batch 12 loss: 0.06451039761304855\n",
            "  batch 13 loss: 0.13185636699199677\n",
            "  batch 14 loss: 0.33515408635139465\n",
            "  batch 15 loss: 0.22420644760131836\n",
            "  batch 16 loss: 0.07755400985479355\n",
            "  batch 17 loss: 0.09885542094707489\n",
            "  batch 18 loss: 0.2023068219423294\n",
            "  batch 19 loss: 0.10700882971286774\n",
            "  batch 20 loss: 0.11324875056743622\n",
            "  batch 21 loss: 0.07257908582687378\n",
            "  batch 22 loss: 0.20409217476844788\n",
            "  batch 23 loss: 0.566072404384613\n",
            "  batch 24 loss: 0.2793331742286682\n",
            "  batch 25 loss: 0.3118225932121277\n",
            "  batch 26 loss: 0.3939565420150757\n",
            "  batch 27 loss: 0.1360674649477005\n",
            "  batch 28 loss: 0.2169790267944336\n",
            "  batch 29 loss: 0.2254447638988495\n",
            "  batch 30 loss: 0.9809379577636719\n",
            "  batch 31 loss: 0.2066015899181366\n",
            "  batch 32 loss: 0.05157874524593353\n",
            "  batch 33 loss: 0.12157461047172546\n",
            "  batch 34 loss: 0.14592036604881287\n",
            "  batch 35 loss: 0.1795501410961151\n",
            "  batch 36 loss: 0.12679702043533325\n",
            "  batch 37 loss: 0.1836317777633667\n",
            "  batch 38 loss: 0.11846044659614563\n",
            "  batch 39 loss: 0.17234669625759125\n",
            "  batch 40 loss: 0.243826761841774\n",
            "  batch 41 loss: 0.2287544310092926\n",
            "  batch 42 loss: 0.1445033848285675\n",
            "  batch 43 loss: 0.2065412700176239\n",
            "  batch 44 loss: 0.1416776478290558\n",
            "  batch 45 loss: 0.09747546911239624\n",
            "  batch 46 loss: 0.14536377787590027\n",
            "  batch 47 loss: 0.7071830034255981\n",
            "  batch 48 loss: 0.07589413225650787\n",
            "  batch 49 loss: 0.16767588257789612\n",
            "  batch 50 loss: 0.10917206108570099\n",
            "  batch 51 loss: 0.09780819714069366\n",
            "  batch 52 loss: 0.16032621264457703\n",
            "  batch 53 loss: 0.25414738059043884\n",
            "  batch 54 loss: 0.16318561136722565\n",
            "  batch 55 loss: 0.1442195177078247\n",
            "  batch 56 loss: 0.3988358974456787\n",
            "  batch 57 loss: 0.006736352574080229\n",
            "  batch 58 loss: 0.12121271342039108\n",
            "  batch 59 loss: 0.22222207486629486\n",
            "  batch 60 loss: 0.2816198170185089\n",
            "  batch 61 loss: 0.2869285047054291\n",
            "  batch 62 loss: 0.3200353682041168\n",
            "  batch 63 loss: 0.4074711203575134\n",
            "  batch 64 loss: 0.3620343804359436\n",
            "  batch 65 loss: 0.013318037614226341\n",
            "  batch 66 loss: 0.07475873827934265\n",
            "  batch 67 loss: 0.31639036536216736\n",
            "  batch 68 loss: 0.6321575045585632\n",
            "  batch 69 loss: 0.24042004346847534\n",
            "  batch 70 loss: 0.157826766371727\n",
            "  batch 71 loss: 0.17981074750423431\n",
            "  batch 72 loss: 0.1773010492324829\n",
            "  batch 73 loss: 0.25442150235176086\n",
            "  batch 74 loss: 0.18926002085208893\n",
            "  batch 75 loss: 0.2559533715248108\n",
            "  batch 76 loss: 0.14850620925426483\n",
            "  batch 77 loss: 0.1985529661178589\n",
            "  batch 78 loss: 0.07744991779327393\n",
            "  batch 79 loss: 0.11437183618545532\n",
            "  batch 80 loss: 0.00836117658764124\n",
            "LOSS train 0.00836117658764124 valid 1.0281906127929688\n",
            "EPOCH 11:\n",
            "  batch 1 loss: 0.03587977588176727\n",
            "  batch 2 loss: 0.3005557954311371\n",
            "  batch 3 loss: 0.15280234813690186\n",
            "  batch 4 loss: 0.10085373371839523\n",
            "  batch 5 loss: 0.20755714178085327\n",
            "  batch 6 loss: 0.07543309777975082\n",
            "  batch 7 loss: 0.2686276435852051\n",
            "  batch 8 loss: 0.14236381649971008\n",
            "  batch 9 loss: 0.43772876262664795\n",
            "  batch 10 loss: 0.18571875989437103\n",
            "  batch 11 loss: 0.21873414516448975\n",
            "  batch 12 loss: 0.2158779501914978\n",
            "  batch 13 loss: 0.06363221257925034\n",
            "  batch 14 loss: 0.10248763859272003\n",
            "  batch 15 loss: 0.2756471633911133\n",
            "  batch 16 loss: 0.11915899813175201\n",
            "  batch 17 loss: 0.3168933391571045\n",
            "  batch 18 loss: 0.14055541157722473\n",
            "  batch 19 loss: 0.2650250792503357\n",
            "  batch 20 loss: 0.4698856472969055\n",
            "  batch 21 loss: 0.36445727944374084\n",
            "  batch 22 loss: 0.06825941801071167\n",
            "  batch 23 loss: 0.08419325947761536\n",
            "  batch 24 loss: 0.1676800400018692\n",
            "  batch 25 loss: 0.6985547542572021\n",
            "  batch 26 loss: 0.06600456684827805\n",
            "  batch 27 loss: 0.28911009430885315\n",
            "  batch 28 loss: 0.28491055965423584\n",
            "  batch 29 loss: 0.20679906010627747\n",
            "  batch 30 loss: 0.26960489153862\n",
            "  batch 31 loss: 0.14702215790748596\n",
            "  batch 32 loss: 0.31274446845054626\n",
            "  batch 33 loss: 0.005013433750718832\n",
            "  batch 34 loss: 0.29886317253112793\n",
            "  batch 35 loss: 0.281871497631073\n",
            "  batch 36 loss: 0.2843612730503082\n",
            "  batch 37 loss: 0.23911550641059875\n",
            "  batch 38 loss: 0.11217690259218216\n",
            "  batch 39 loss: 0.04776887968182564\n",
            "  batch 40 loss: 0.2770482301712036\n",
            "  batch 41 loss: 0.1340971738100052\n",
            "  batch 42 loss: 0.3133614957332611\n",
            "  batch 43 loss: 0.1513393521308899\n",
            "  batch 44 loss: 0.07238692790269852\n",
            "  batch 45 loss: 0.1069164127111435\n",
            "  batch 46 loss: 0.6156347990036011\n",
            "  batch 47 loss: 0.2139829397201538\n",
            "  batch 48 loss: 0.583320140838623\n",
            "  batch 49 loss: 0.06899600476026535\n",
            "  batch 50 loss: 0.5563135743141174\n",
            "  batch 51 loss: 0.06939505785703659\n",
            "  batch 52 loss: 0.12449080497026443\n",
            "  batch 53 loss: 0.13698294758796692\n",
            "  batch 54 loss: 0.37733253836631775\n",
            "  batch 55 loss: 0.22087249159812927\n",
            "  batch 56 loss: 0.15412944555282593\n",
            "  batch 57 loss: 0.13042999804019928\n",
            "  batch 58 loss: 0.042449597269296646\n",
            "  batch 59 loss: 0.3398700952529907\n",
            "  batch 60 loss: 0.1306905448436737\n",
            "  batch 61 loss: 0.17105209827423096\n",
            "  batch 62 loss: 0.3020683526992798\n",
            "  batch 63 loss: 0.27027013897895813\n",
            "  batch 64 loss: 0.10809138417243958\n",
            "  batch 65 loss: 0.14404603838920593\n",
            "  batch 66 loss: 0.08387038111686707\n",
            "  batch 67 loss: 0.04768770933151245\n",
            "  batch 68 loss: 0.29713720083236694\n",
            "  batch 69 loss: 0.16491512954235077\n",
            "  batch 70 loss: 0.04432884231209755\n",
            "  batch 71 loss: 0.18555720150470734\n",
            "  batch 72 loss: 0.1984265297651291\n",
            "  batch 73 loss: 0.45879554748535156\n",
            "  batch 74 loss: 0.0952146127820015\n",
            "  batch 75 loss: 0.08083060383796692\n",
            "  batch 76 loss: 0.09893955290317535\n",
            "  batch 77 loss: 0.29361048340797424\n",
            "  batch 78 loss: 0.19990992546081543\n",
            "  batch 79 loss: 0.3331664800643921\n",
            "  batch 80 loss: 0.0799124538898468\n",
            "LOSS train 0.0799124538898468 valid 1.1044286489486694\n",
            "EPOCH 12:\n",
            "  batch 1 loss: 0.26766449213027954\n",
            "  batch 2 loss: 0.19566723704338074\n",
            "  batch 3 loss: 0.04654993116855621\n",
            "  batch 4 loss: 0.20679785311222076\n",
            "  batch 5 loss: 0.062109559774398804\n",
            "  batch 6 loss: 0.5707427859306335\n",
            "  batch 7 loss: 0.04797269031405449\n",
            "  batch 8 loss: 0.13391411304473877\n",
            "  batch 9 loss: 0.07494863867759705\n",
            "  batch 10 loss: 0.10873380303382874\n",
            "  batch 11 loss: 0.15726406872272491\n",
            "  batch 12 loss: 0.11200487613677979\n",
            "  batch 13 loss: 0.2236068844795227\n",
            "  batch 14 loss: 0.025487113744020462\n",
            "  batch 15 loss: 0.21820604801177979\n",
            "  batch 16 loss: 0.04259149730205536\n",
            "  batch 17 loss: 0.6718267798423767\n",
            "  batch 18 loss: 0.09137164801359177\n",
            "  batch 19 loss: 0.17321142554283142\n",
            "  batch 20 loss: 0.39870697259902954\n",
            "  batch 21 loss: 0.1280335932970047\n",
            "  batch 22 loss: 0.23796895146369934\n",
            "  batch 23 loss: 0.10474122315645218\n",
            "  batch 24 loss: 0.29950395226478577\n",
            "  batch 25 loss: 0.15246883034706116\n",
            "  batch 26 loss: 0.2934952974319458\n",
            "  batch 27 loss: 0.17992734909057617\n",
            "  batch 28 loss: 0.25020402669906616\n",
            "  batch 29 loss: 0.07795124500989914\n",
            "  batch 30 loss: 0.0741945132613182\n",
            "  batch 31 loss: 0.16899418830871582\n",
            "  batch 32 loss: 0.17951951920986176\n",
            "  batch 33 loss: 0.07106764614582062\n",
            "  batch 34 loss: 0.02300606667995453\n",
            "  batch 35 loss: 0.12230880558490753\n",
            "  batch 36 loss: 0.25774896144866943\n",
            "  batch 37 loss: 0.06874711066484451\n",
            "  batch 38 loss: 0.11587017774581909\n",
            "  batch 39 loss: 0.07716544717550278\n",
            "  batch 40 loss: 0.4168013036251068\n",
            "  batch 41 loss: 0.7187890410423279\n",
            "  batch 42 loss: 0.2468058168888092\n",
            "  batch 43 loss: 0.1876823455095291\n",
            "  batch 44 loss: 0.19504159688949585\n",
            "  batch 45 loss: 0.2735588550567627\n",
            "  batch 46 loss: 0.25792136788368225\n",
            "  batch 47 loss: 0.11844061315059662\n",
            "  batch 48 loss: 0.272623211145401\n",
            "  batch 49 loss: 0.28329622745513916\n",
            "  batch 50 loss: 0.20910075306892395\n",
            "  batch 51 loss: 0.2929566204547882\n",
            "  batch 52 loss: 0.2677914500236511\n",
            "  batch 53 loss: 0.2993929982185364\n",
            "  batch 54 loss: 0.05629982054233551\n",
            "  batch 55 loss: 0.2586694061756134\n",
            "  batch 56 loss: 0.13474440574645996\n",
            "  batch 57 loss: 0.28402698040008545\n",
            "  batch 58 loss: 0.0739806592464447\n",
            "  batch 59 loss: 0.2034626007080078\n",
            "  batch 60 loss: 0.16765998303890228\n",
            "  batch 61 loss: 0.0917484313249588\n",
            "  batch 62 loss: 0.2868838906288147\n",
            "  batch 63 loss: 0.37466225028038025\n",
            "  batch 64 loss: 0.18812420964241028\n",
            "  batch 65 loss: 0.02862812578678131\n",
            "  batch 66 loss: 0.1164737343788147\n",
            "  batch 67 loss: 0.05234172195196152\n",
            "  batch 68 loss: 0.44671279191970825\n",
            "  batch 69 loss: 0.07436053454875946\n",
            "  batch 70 loss: 0.33272093534469604\n",
            "  batch 71 loss: 0.10248148441314697\n",
            "  batch 72 loss: 0.5811057686805725\n",
            "  batch 73 loss: 0.36801591515541077\n",
            "  batch 74 loss: 0.3589729070663452\n",
            "  batch 75 loss: 0.23666150867938995\n",
            "  batch 76 loss: 0.37330907583236694\n",
            "  batch 77 loss: 0.2522991895675659\n",
            "  batch 78 loss: 0.1758079081773758\n",
            "  batch 79 loss: 0.28711462020874023\n",
            "  batch 80 loss: 0.45896512269973755\n",
            "LOSS train 0.45896512269973755 valid 1.1126166582107544\n",
            "EPOCH 13:\n",
            "  batch 1 loss: 0.08897177875041962\n",
            "  batch 2 loss: 0.12304730713367462\n",
            "  batch 3 loss: 0.11593955755233765\n",
            "  batch 4 loss: 0.4321899116039276\n",
            "  batch 5 loss: 0.2813362777233124\n",
            "  batch 6 loss: 0.7498572468757629\n",
            "  batch 7 loss: 0.1310075968503952\n",
            "  batch 8 loss: 0.255790114402771\n",
            "  batch 9 loss: 0.27411893010139465\n",
            "  batch 10 loss: 0.12791137397289276\n",
            "  batch 11 loss: 0.26655083894729614\n",
            "  batch 12 loss: 0.17507222294807434\n",
            "  batch 13 loss: 0.16674652695655823\n",
            "  batch 14 loss: 0.4940127432346344\n",
            "  batch 15 loss: 0.3482389748096466\n",
            "  batch 16 loss: 0.2695380449295044\n",
            "  batch 17 loss: 0.43594688177108765\n",
            "  batch 18 loss: 0.6417270302772522\n",
            "  batch 19 loss: 0.25897711515426636\n",
            "  batch 20 loss: 0.21812321245670319\n",
            "  batch 21 loss: 0.06201929971575737\n",
            "  batch 22 loss: 0.3099576234817505\n",
            "  batch 23 loss: 0.15723523497581482\n",
            "  batch 24 loss: 0.15208713710308075\n",
            "  batch 25 loss: 0.2792345881462097\n",
            "  batch 26 loss: 0.04285912960767746\n",
            "  batch 27 loss: 0.14927947521209717\n",
            "  batch 28 loss: 0.017601095139980316\n",
            "  batch 29 loss: 0.2223616987466812\n",
            "  batch 30 loss: 0.06993120163679123\n",
            "  batch 31 loss: 0.1565694510936737\n",
            "  batch 32 loss: 0.2977798283100128\n",
            "  batch 33 loss: 0.10986766219139099\n",
            "  batch 34 loss: 0.10926371812820435\n",
            "  batch 35 loss: 0.12089786678552628\n",
            "  batch 36 loss: 0.44142061471939087\n",
            "  batch 37 loss: 0.10885076224803925\n",
            "  batch 38 loss: 0.08691847324371338\n",
            "  batch 39 loss: 0.09153952449560165\n",
            "  batch 40 loss: 0.2664702534675598\n",
            "  batch 41 loss: 0.022449137642979622\n",
            "  batch 42 loss: 0.04890580475330353\n",
            "  batch 43 loss: 0.1144225001335144\n",
            "  batch 44 loss: 0.263719379901886\n",
            "  batch 45 loss: 0.02845723181962967\n",
            "  batch 46 loss: 0.1116839125752449\n",
            "  batch 47 loss: 0.31213417649269104\n",
            "  batch 48 loss: 0.379100501537323\n",
            "  batch 49 loss: 0.0527825802564621\n",
            "  batch 50 loss: 0.06366878747940063\n",
            "  batch 51 loss: 0.250998318195343\n",
            "  batch 52 loss: 0.10645991563796997\n",
            "  batch 53 loss: 0.12277896702289581\n",
            "  batch 54 loss: 0.046626977622509\n",
            "  batch 55 loss: 0.22993606328964233\n",
            "  batch 56 loss: 0.24800126254558563\n",
            "  batch 57 loss: 0.201983243227005\n",
            "  batch 58 loss: 0.16770032048225403\n",
            "  batch 59 loss: 0.04194020852446556\n",
            "  batch 60 loss: 0.3517453968524933\n",
            "  batch 61 loss: 0.1264665722846985\n",
            "  batch 62 loss: 0.4798499345779419\n",
            "  batch 63 loss: 0.24821721017360687\n",
            "  batch 64 loss: 0.1746039241552353\n",
            "  batch 65 loss: 0.2713681757450104\n",
            "  batch 66 loss: 0.6425300240516663\n",
            "  batch 67 loss: 0.03093242086470127\n",
            "  batch 68 loss: 0.23498904705047607\n",
            "  batch 69 loss: 0.3633468747138977\n",
            "  batch 70 loss: 0.1197846308350563\n",
            "  batch 71 loss: 0.11727949976921082\n",
            "  batch 72 loss: 0.24652710556983948\n",
            "  batch 73 loss: 0.211571604013443\n",
            "  batch 74 loss: 0.0911477655172348\n",
            "  batch 75 loss: 0.10436899960041046\n",
            "  batch 76 loss: 0.19695064425468445\n",
            "  batch 77 loss: 0.006769801490008831\n",
            "  batch 78 loss: 0.03386402875185013\n",
            "  batch 79 loss: 0.3230701684951782\n",
            "  batch 80 loss: 0.24461360275745392\n",
            "LOSS train 0.24461360275745392 valid 1.1671853065490723\n",
            "EPOCH 14:\n",
            "  batch 1 loss: 0.193613201379776\n",
            "  batch 2 loss: 0.3392234146595001\n",
            "  batch 3 loss: 0.03871126100420952\n",
            "  batch 4 loss: 0.28957101702690125\n",
            "  batch 5 loss: 0.2136528193950653\n",
            "  batch 6 loss: 0.1867462396621704\n",
            "  batch 7 loss: 0.09347809851169586\n",
            "  batch 8 loss: 0.25522345304489136\n",
            "  batch 9 loss: 0.19664792716503143\n",
            "  batch 10 loss: 0.29744240641593933\n",
            "  batch 11 loss: 0.09137635678052902\n",
            "  batch 12 loss: 0.021759649738669395\n",
            "  batch 13 loss: 0.04369639977812767\n",
            "  batch 14 loss: 0.15417155623435974\n",
            "  batch 15 loss: 0.35885241627693176\n",
            "  batch 16 loss: 0.22590570151805878\n",
            "  batch 17 loss: 0.11828231811523438\n",
            "  batch 18 loss: 0.10297629982233047\n",
            "  batch 19 loss: 0.23721152544021606\n",
            "  batch 20 loss: 0.16578309237957\n",
            "  batch 21 loss: 0.06501825898885727\n",
            "  batch 22 loss: 0.1113826110959053\n",
            "  batch 23 loss: 0.01685568317770958\n",
            "  batch 24 loss: 0.29408347606658936\n",
            "  batch 25 loss: 0.27459752559661865\n",
            "  batch 26 loss: 0.34726253151893616\n",
            "  batch 27 loss: 0.07779593020677567\n",
            "  batch 28 loss: 0.08044625073671341\n",
            "  batch 29 loss: 0.1449684500694275\n",
            "  batch 30 loss: 0.20945510268211365\n",
            "  batch 31 loss: 0.163798987865448\n",
            "  batch 32 loss: 0.25795063376426697\n",
            "  batch 33 loss: 0.162045419216156\n",
            "  batch 34 loss: 0.25459781289100647\n",
            "  batch 35 loss: 0.47582733631134033\n",
            "  batch 36 loss: 0.13121344149112701\n",
            "  batch 37 loss: 0.0761917233467102\n",
            "  batch 38 loss: 0.06506951153278351\n",
            "  batch 39 loss: 0.3257230520248413\n",
            "  batch 40 loss: 0.1665918081998825\n",
            "  batch 41 loss: 0.4452221095561981\n",
            "  batch 42 loss: 0.04696085304021835\n",
            "  batch 43 loss: 0.2900446951389313\n",
            "  batch 44 loss: 0.2009618878364563\n",
            "  batch 45 loss: 0.05935991555452347\n",
            "  batch 46 loss: 0.19958192110061646\n",
            "  batch 47 loss: 0.23676340281963348\n",
            "  batch 48 loss: 0.7191529870033264\n",
            "  batch 49 loss: 0.14084011316299438\n",
            "  batch 50 loss: 0.12299961596727371\n",
            "  batch 51 loss: 0.293725848197937\n",
            "  batch 52 loss: 0.3270522952079773\n",
            "  batch 53 loss: 0.1382485181093216\n",
            "  batch 54 loss: 0.28723639249801636\n",
            "  batch 55 loss: 0.03258521109819412\n",
            "  batch 56 loss: 0.47562313079833984\n",
            "  batch 57 loss: 0.034715309739112854\n",
            "  batch 58 loss: 0.23938250541687012\n",
            "  batch 59 loss: 0.13003790378570557\n",
            "  batch 60 loss: 0.5336579084396362\n",
            "  batch 61 loss: 0.14204959571361542\n",
            "  batch 62 loss: 0.1873224824666977\n",
            "  batch 63 loss: 0.2011221945285797\n",
            "  batch 64 loss: 0.1634541004896164\n",
            "  batch 65 loss: 0.12216392159461975\n",
            "  batch 66 loss: 0.6559364199638367\n",
            "  batch 67 loss: 0.041300803422927856\n",
            "  batch 68 loss: 0.09458308666944504\n",
            "  batch 69 loss: 0.2751421332359314\n",
            "  batch 70 loss: 0.11552207171916962\n",
            "  batch 71 loss: 0.27122801542282104\n",
            "  batch 72 loss: 0.49399444460868835\n",
            "  batch 73 loss: 0.06403128057718277\n",
            "  batch 74 loss: 0.08411653339862823\n",
            "  batch 75 loss: 0.08028511703014374\n",
            "  batch 76 loss: 0.21422868967056274\n",
            "  batch 77 loss: 0.013318425044417381\n",
            "  batch 78 loss: 0.17767123878002167\n",
            "  batch 79 loss: 0.10372404009103775\n",
            "  batch 80 loss: 0.9546120166778564\n",
            "LOSS train 0.9546120166778564 valid 1.073184847831726\n",
            "EPOCH 15:\n",
            "  batch 1 loss: 0.17723619937896729\n",
            "  batch 2 loss: 0.29884278774261475\n",
            "  batch 3 loss: 0.2796458899974823\n",
            "  batch 4 loss: 0.5401928424835205\n",
            "  batch 5 loss: 0.24768543243408203\n",
            "  batch 6 loss: 0.014524978585541248\n",
            "  batch 7 loss: 0.10951186716556549\n",
            "  batch 8 loss: 0.18163999915122986\n",
            "  batch 9 loss: 0.5767660140991211\n",
            "  batch 10 loss: 0.2578691840171814\n",
            "  batch 11 loss: 0.3656931221485138\n",
            "  batch 12 loss: 0.27164122462272644\n",
            "  batch 13 loss: 0.2937483489513397\n",
            "  batch 14 loss: 0.016650255769491196\n",
            "  batch 15 loss: 0.172177255153656\n",
            "  batch 16 loss: 0.2824392020702362\n",
            "  batch 17 loss: 0.20349878072738647\n",
            "  batch 18 loss: 0.021180493757128716\n",
            "  batch 19 loss: 0.2053360491991043\n",
            "  batch 20 loss: 0.04307332634925842\n",
            "  batch 21 loss: 0.19566664099693298\n",
            "  batch 22 loss: 0.3686821162700653\n",
            "  batch 23 loss: 0.05915405601263046\n",
            "  batch 24 loss: 0.11626226454973221\n",
            "  batch 25 loss: 0.07339740544557571\n",
            "  batch 26 loss: 0.11319668591022491\n",
            "  batch 27 loss: 0.07194630801677704\n",
            "  batch 28 loss: 0.16312089562416077\n",
            "  batch 29 loss: 0.5133854150772095\n",
            "  batch 30 loss: 0.23681409657001495\n",
            "  batch 31 loss: 0.5231500864028931\n",
            "  batch 32 loss: 0.25521838665008545\n",
            "  batch 33 loss: 0.09719260036945343\n",
            "  batch 34 loss: 0.19195063412189484\n",
            "  batch 35 loss: 0.1571369767189026\n",
            "  batch 36 loss: 0.46085429191589355\n",
            "  batch 37 loss: 0.15245263278484344\n",
            "  batch 38 loss: 0.23878604173660278\n",
            "  batch 39 loss: 0.08250655233860016\n",
            "  batch 40 loss: 0.05037905275821686\n",
            "  batch 41 loss: 0.09696182608604431\n",
            "  batch 42 loss: 0.9504149556159973\n",
            "  batch 43 loss: 0.15734796226024628\n",
            "  batch 44 loss: 0.25808221101760864\n",
            "  batch 45 loss: 0.3002265691757202\n",
            "  batch 46 loss: 0.20559847354888916\n",
            "  batch 47 loss: 0.34604552388191223\n",
            "  batch 48 loss: 0.07966361939907074\n",
            "  batch 49 loss: 0.045805491507053375\n",
            "  batch 50 loss: 0.24732095003128052\n",
            "  batch 51 loss: 0.09931746870279312\n",
            "  batch 52 loss: 0.04305148124694824\n",
            "  batch 53 loss: 0.12329500913619995\n",
            "  batch 54 loss: 0.2825782001018524\n",
            "  batch 55 loss: 0.11207808554172516\n",
            "  batch 56 loss: 0.6226282119750977\n",
            "  batch 57 loss: 0.03938200697302818\n",
            "  batch 58 loss: 0.13395145535469055\n",
            "  batch 59 loss: 0.11117694526910782\n",
            "  batch 60 loss: 0.20272867381572723\n",
            "  batch 61 loss: 0.2364213764667511\n",
            "  batch 62 loss: 0.03958781436085701\n",
            "  batch 63 loss: 0.10170376300811768\n",
            "  batch 64 loss: 0.08017158508300781\n",
            "  batch 65 loss: 0.2959509789943695\n",
            "  batch 66 loss: 0.024124499410390854\n",
            "  batch 67 loss: 0.09023728221654892\n",
            "  batch 68 loss: 0.15609602630138397\n",
            "  batch 69 loss: 0.2506726384162903\n",
            "  batch 70 loss: 0.15104293823242188\n",
            "  batch 71 loss: 0.0677797719836235\n",
            "  batch 72 loss: 0.29745417833328247\n",
            "  batch 73 loss: 0.25394225120544434\n",
            "  batch 74 loss: 0.2679865062236786\n",
            "  batch 75 loss: 0.19481150805950165\n",
            "  batch 76 loss: 0.22410380840301514\n",
            "  batch 77 loss: 0.19078990817070007\n",
            "  batch 78 loss: 0.09947821497917175\n",
            "  batch 79 loss: 0.11906547099351883\n",
            "  batch 80 loss: 0.20301437377929688\n",
            "LOSS train 0.20301437377929688 valid 1.0974887609481812\n",
            "EPOCH 16:\n",
            "  batch 1 loss: 0.014223302714526653\n",
            "  batch 2 loss: 0.20516419410705566\n",
            "  batch 3 loss: 0.11733993142843246\n",
            "  batch 4 loss: 0.041335344314575195\n",
            "  batch 5 loss: 0.19044634699821472\n",
            "  batch 6 loss: 0.11319290846586227\n",
            "  batch 7 loss: 0.2621415853500366\n",
            "  batch 8 loss: 0.08577240258455276\n",
            "  batch 9 loss: 0.45878663659095764\n",
            "  batch 10 loss: 0.21404270827770233\n",
            "  batch 11 loss: 0.011362838558852673\n",
            "  batch 12 loss: 0.1762847900390625\n",
            "  batch 13 loss: 0.18857648968696594\n",
            "  batch 14 loss: 0.09890962392091751\n",
            "  batch 15 loss: 0.08962918817996979\n",
            "  batch 16 loss: 0.09605205804109573\n",
            "  batch 17 loss: 0.09846098721027374\n",
            "  batch 18 loss: 0.5544179677963257\n",
            "  batch 19 loss: 0.37835144996643066\n",
            "  batch 20 loss: 0.1175113320350647\n",
            "  batch 21 loss: 0.00810103490948677\n",
            "  batch 22 loss: 0.20747780799865723\n",
            "  batch 23 loss: 0.7512732148170471\n",
            "  batch 24 loss: 0.007150882389396429\n",
            "  batch 25 loss: 0.3579210638999939\n",
            "  batch 26 loss: 0.33120766282081604\n",
            "  batch 27 loss: 0.22071602940559387\n",
            "  batch 28 loss: 0.06141624227166176\n",
            "  batch 29 loss: 0.11866207420825958\n",
            "  batch 30 loss: 0.24878822267055511\n",
            "  batch 31 loss: 0.2457537055015564\n",
            "  batch 32 loss: 0.2590136229991913\n",
            "  batch 33 loss: 0.1459466814994812\n",
            "  batch 34 loss: 0.4294340908527374\n",
            "  batch 35 loss: 0.22381915152072906\n",
            "  batch 36 loss: 0.14996397495269775\n",
            "  batch 37 loss: 0.6802929043769836\n",
            "  batch 38 loss: 0.06944126635789871\n",
            "  batch 39 loss: 0.22578026354312897\n",
            "  batch 40 loss: 0.171939417719841\n",
            "  batch 41 loss: 0.07702381163835526\n",
            "  batch 42 loss: 0.07536874711513519\n",
            "  batch 43 loss: 0.20494145154953003\n",
            "  batch 44 loss: 0.18081223964691162\n",
            "  batch 45 loss: 0.351296603679657\n",
            "  batch 46 loss: 0.3959243595600128\n",
            "  batch 47 loss: 0.07665948569774628\n",
            "  batch 48 loss: 0.2262984812259674\n",
            "  batch 49 loss: 0.17863623797893524\n",
            "  batch 50 loss: 0.31038981676101685\n",
            "  batch 51 loss: 0.13242481648921967\n",
            "  batch 52 loss: 0.08351867645978928\n",
            "  batch 53 loss: 0.08938835561275482\n",
            "  batch 54 loss: 0.06106206029653549\n",
            "  batch 55 loss: 0.05591505393385887\n",
            "  batch 56 loss: 0.1543886959552765\n",
            "  batch 57 loss: 0.10973527282476425\n",
            "  batch 58 loss: 0.18445996940135956\n",
            "  batch 59 loss: 0.12200555205345154\n",
            "  batch 60 loss: 0.2760545611381531\n",
            "  batch 61 loss: 0.009209847077727318\n",
            "  batch 62 loss: 0.02367301657795906\n",
            "  batch 63 loss: 0.1864841729402542\n",
            "  batch 64 loss: 0.1680564433336258\n",
            "  batch 65 loss: 0.1598019301891327\n",
            "  batch 66 loss: 0.39539834856987\n",
            "  batch 67 loss: 0.31670188903808594\n",
            "  batch 68 loss: 0.13272418081760406\n",
            "  batch 69 loss: 0.28596216440200806\n",
            "  batch 70 loss: 0.17121011018753052\n",
            "  batch 71 loss: 0.12248918414115906\n",
            "  batch 72 loss: 0.0975390076637268\n",
            "  batch 73 loss: 0.19635504484176636\n",
            "  batch 74 loss: 0.13922549784183502\n",
            "  batch 75 loss: 0.21213774383068085\n",
            "  batch 76 loss: 0.5829564332962036\n",
            "  batch 77 loss: 0.10675640404224396\n",
            "  batch 78 loss: 0.24229590594768524\n",
            "  batch 79 loss: 0.7538067102432251\n",
            "  batch 80 loss: 0.060058724135160446\n",
            "LOSS train 0.060058724135160446 valid 1.1189494132995605\n",
            "EPOCH 17:\n",
            "  batch 1 loss: 0.10095196217298508\n",
            "  batch 2 loss: 0.3001086711883545\n",
            "  batch 3 loss: 0.37885212898254395\n",
            "  batch 4 loss: 0.1364232450723648\n",
            "  batch 5 loss: 0.21690633893013\n",
            "  batch 6 loss: 0.5639682412147522\n",
            "  batch 7 loss: 0.19714173674583435\n",
            "  batch 8 loss: 0.2762787640094757\n",
            "  batch 9 loss: 0.37443968653678894\n",
            "  batch 10 loss: 0.21756428480148315\n",
            "  batch 11 loss: 0.046663057059049606\n",
            "  batch 12 loss: 0.11027948558330536\n",
            "  batch 13 loss: 0.08270315080881119\n",
            "  batch 14 loss: 0.1499185711145401\n",
            "  batch 15 loss: 0.1854865700006485\n",
            "  batch 16 loss: 0.10199245810508728\n",
            "  batch 17 loss: 0.43991705775260925\n",
            "  batch 18 loss: 0.14824749529361725\n",
            "  batch 19 loss: 0.017657190561294556\n",
            "  batch 20 loss: 0.09040582180023193\n",
            "  batch 21 loss: 0.20513007044792175\n",
            "  batch 22 loss: 0.12759637832641602\n",
            "  batch 23 loss: 0.12611690163612366\n",
            "  batch 24 loss: 0.109336718916893\n",
            "  batch 25 loss: 0.28677451610565186\n",
            "  batch 26 loss: 0.06543999165296555\n",
            "  batch 27 loss: 0.1438198834657669\n",
            "  batch 28 loss: 0.30551499128341675\n",
            "  batch 29 loss: 0.19923508167266846\n",
            "  batch 30 loss: 0.17179173231124878\n",
            "  batch 31 loss: 0.16415385901927948\n",
            "  batch 32 loss: 0.33474424481391907\n",
            "  batch 33 loss: 0.22043085098266602\n",
            "  batch 34 loss: 0.16050153970718384\n",
            "  batch 35 loss: 0.15714137256145477\n",
            "  batch 36 loss: 0.12357938289642334\n",
            "  batch 37 loss: 0.23383468389511108\n",
            "  batch 38 loss: 0.26439693570137024\n",
            "  batch 39 loss: 0.18686872720718384\n",
            "  batch 40 loss: 0.07285965234041214\n",
            "  batch 41 loss: 0.2314232736825943\n",
            "  batch 42 loss: 0.14470896124839783\n",
            "  batch 43 loss: 0.19478096067905426\n",
            "  batch 44 loss: 0.14229623973369598\n",
            "  batch 45 loss: 0.15943829715251923\n",
            "  batch 46 loss: 0.0866202637553215\n",
            "  batch 47 loss: 0.2149980515241623\n",
            "  batch 48 loss: 0.06572062522172928\n",
            "  batch 49 loss: 0.04974593594670296\n",
            "  batch 50 loss: 0.38822826743125916\n",
            "  batch 51 loss: 0.08722611516714096\n",
            "  batch 52 loss: 0.17595572769641876\n",
            "  batch 53 loss: 0.5287400484085083\n",
            "  batch 54 loss: 0.8046932220458984\n",
            "  batch 55 loss: 0.13572636246681213\n",
            "  batch 56 loss: 0.21267187595367432\n",
            "  batch 57 loss: 0.6585092544555664\n",
            "  batch 58 loss: 0.1603839099407196\n",
            "  batch 59 loss: 0.03129718825221062\n",
            "  batch 60 loss: 0.03484579175710678\n",
            "  batch 61 loss: 0.16421473026275635\n",
            "  batch 62 loss: 0.2967827022075653\n",
            "  batch 63 loss: 0.111075259745121\n",
            "  batch 64 loss: 0.2405681610107422\n",
            "  batch 65 loss: 0.12911787629127502\n",
            "  batch 66 loss: 0.09734498709440231\n",
            "  batch 67 loss: 0.7103274464607239\n",
            "  batch 68 loss: 0.3858211636543274\n",
            "  batch 69 loss: 0.10896933078765869\n",
            "  batch 70 loss: 0.10802427679300308\n",
            "  batch 71 loss: 0.19085998833179474\n",
            "  batch 72 loss: 0.11222820729017258\n",
            "  batch 73 loss: 0.07519659399986267\n",
            "  batch 74 loss: 0.09385518729686737\n",
            "  batch 75 loss: 0.24904030561447144\n",
            "  batch 76 loss: 0.09152292460203171\n",
            "  batch 77 loss: 0.21125081181526184\n",
            "  batch 78 loss: 0.4419395327568054\n",
            "  batch 79 loss: 0.016892727464437485\n",
            "  batch 80 loss: 0.03192094713449478\n",
            "LOSS train 0.03192094713449478 valid 1.0325844287872314\n",
            "EPOCH 18:\n",
            "  batch 1 loss: 0.2747679054737091\n",
            "  batch 2 loss: 0.2144073247909546\n",
            "  batch 3 loss: 0.21392075717449188\n",
            "  batch 4 loss: 0.34771212935447693\n",
            "  batch 5 loss: 0.14169317483901978\n",
            "  batch 6 loss: 0.1876879185438156\n",
            "  batch 7 loss: 0.30353230237960815\n",
            "  batch 8 loss: 0.05134478583931923\n",
            "  batch 9 loss: 0.18099921941757202\n",
            "  batch 10 loss: 0.2712094783782959\n",
            "  batch 11 loss: 0.13125289976596832\n",
            "  batch 12 loss: 0.295903742313385\n",
            "  batch 13 loss: 0.0735376700758934\n",
            "  batch 14 loss: 0.15215915441513062\n",
            "  batch 15 loss: 0.6038603782653809\n",
            "  batch 16 loss: 0.05432513356208801\n",
            "  batch 17 loss: 0.07614346593618393\n",
            "  batch 18 loss: 0.046192631125450134\n",
            "  batch 19 loss: 0.15754547715187073\n",
            "  batch 20 loss: 0.23061546683311462\n",
            "  batch 21 loss: 0.008328667841851711\n",
            "  batch 22 loss: 0.2824709713459015\n",
            "  batch 23 loss: 0.09148222208023071\n",
            "  batch 24 loss: 0.04659207910299301\n",
            "  batch 25 loss: 0.46791955828666687\n",
            "  batch 26 loss: 0.3250129818916321\n",
            "  batch 27 loss: 0.35397425293922424\n",
            "  batch 28 loss: 0.1043853834271431\n",
            "  batch 29 loss: 0.6317797303199768\n",
            "  batch 30 loss: 0.5063735246658325\n",
            "  batch 31 loss: 0.23091177642345428\n",
            "  batch 32 loss: 0.21467465162277222\n",
            "  batch 33 loss: 0.12578782439231873\n",
            "  batch 34 loss: 0.14712797105312347\n",
            "  batch 35 loss: 0.02513919398188591\n",
            "  batch 36 loss: 0.041914552450180054\n",
            "  batch 37 loss: 0.18856720626354218\n",
            "  batch 38 loss: 0.07258837670087814\n",
            "  batch 39 loss: 0.0992271900177002\n",
            "  batch 40 loss: 0.06500831991434097\n",
            "  batch 41 loss: 0.08557315170764923\n",
            "  batch 42 loss: 0.16826777160167694\n",
            "  batch 43 loss: 0.04225251078605652\n",
            "  batch 44 loss: 0.4104502499103546\n",
            "  batch 45 loss: 0.06191715970635414\n",
            "  batch 46 loss: 0.15868479013442993\n",
            "  batch 47 loss: 0.18585580587387085\n",
            "  batch 48 loss: 0.10857779532670975\n",
            "  batch 49 loss: 0.18647488951683044\n",
            "  batch 50 loss: 0.08507947623729706\n",
            "  batch 51 loss: 0.144586980342865\n",
            "  batch 52 loss: 0.6225746870040894\n",
            "  batch 53 loss: 0.2834855020046234\n",
            "  batch 54 loss: 0.23948584496974945\n",
            "  batch 55 loss: 0.09212827682495117\n",
            "  batch 56 loss: 0.08257166296243668\n",
            "  batch 57 loss: 0.05914095789194107\n",
            "  batch 58 loss: 0.3839097023010254\n",
            "  batch 59 loss: 0.25256839394569397\n",
            "  batch 60 loss: 0.04382868483662605\n",
            "  batch 61 loss: 0.1526775062084198\n",
            "  batch 62 loss: 0.5776214003562927\n",
            "  batch 63 loss: 0.11438243836164474\n",
            "  batch 64 loss: 0.35643765330314636\n",
            "  batch 65 loss: 0.24879977107048035\n",
            "  batch 66 loss: 0.08538924902677536\n",
            "  batch 67 loss: 0.33598023653030396\n",
            "  batch 68 loss: 0.12194841355085373\n",
            "  batch 69 loss: 0.04090449959039688\n",
            "  batch 70 loss: 0.1454125940799713\n",
            "  batch 71 loss: 0.2914106249809265\n",
            "  batch 72 loss: 0.11433620750904083\n",
            "  batch 73 loss: 0.11221764236688614\n",
            "  batch 74 loss: 0.028545331209897995\n",
            "  batch 75 loss: 0.23916512727737427\n",
            "  batch 76 loss: 0.1877041757106781\n",
            "  batch 77 loss: 0.21553601324558258\n",
            "  batch 78 loss: 0.707000732421875\n",
            "  batch 79 loss: 0.09997670352458954\n",
            "  batch 80 loss: 0.39930275082588196\n",
            "LOSS train 0.39930275082588196 valid 1.0378340482711792\n",
            "EPOCH 19:\n",
            "  batch 1 loss: 0.427726149559021\n",
            "  batch 2 loss: 0.16317325830459595\n",
            "  batch 3 loss: 0.32783743739128113\n",
            "  batch 4 loss: 0.18775728344917297\n",
            "  batch 5 loss: 0.056124426424503326\n",
            "  batch 6 loss: 0.21131451427936554\n",
            "  batch 7 loss: 0.2914772629737854\n",
            "  batch 8 loss: 0.11582889407873154\n",
            "  batch 9 loss: 0.15992118418216705\n",
            "  batch 10 loss: 0.08988514542579651\n",
            "  batch 11 loss: 0.20451535284519196\n",
            "  batch 12 loss: 0.08578265458345413\n",
            "  batch 13 loss: 0.14972397685050964\n",
            "  batch 14 loss: 0.25570914149284363\n",
            "  batch 15 loss: 0.6116716861724854\n",
            "  batch 16 loss: 0.14124874770641327\n",
            "  batch 17 loss: 0.07846841961145401\n",
            "  batch 18 loss: 0.07312038540840149\n",
            "  batch 19 loss: 0.11681227385997772\n",
            "  batch 20 loss: 0.2613356113433838\n",
            "  batch 21 loss: 0.1462646871805191\n",
            "  batch 22 loss: 0.2210935354232788\n",
            "  batch 23 loss: 0.21085244417190552\n",
            "  batch 24 loss: 0.07550819963216782\n",
            "  batch 25 loss: 0.07052087783813477\n",
            "  batch 26 loss: 0.14959397912025452\n",
            "  batch 27 loss: 0.21041101217269897\n",
            "  batch 28 loss: 0.09717714786529541\n",
            "  batch 29 loss: 0.13474000990390778\n",
            "  batch 30 loss: 0.2591083347797394\n",
            "  batch 31 loss: 0.6622422337532043\n",
            "  batch 32 loss: 0.2934914827346802\n",
            "  batch 33 loss: 0.24672599136829376\n",
            "  batch 34 loss: 0.21565565466880798\n",
            "  batch 35 loss: 0.12381552904844284\n",
            "  batch 36 loss: 0.11023833602666855\n",
            "  batch 37 loss: 0.21251103281974792\n",
            "  batch 38 loss: 0.28846848011016846\n",
            "  batch 39 loss: 0.0755906030535698\n",
            "  batch 40 loss: 0.28542235493659973\n",
            "  batch 41 loss: 0.289765864610672\n",
            "  batch 42 loss: 0.10046176612377167\n",
            "  batch 43 loss: 0.21957260370254517\n",
            "  batch 44 loss: 0.3710958957672119\n",
            "  batch 45 loss: 0.6771978139877319\n",
            "  batch 46 loss: 0.11056362092494965\n",
            "  batch 47 loss: 0.13626889884471893\n",
            "  batch 48 loss: 0.24851757287979126\n",
            "  batch 49 loss: 0.09700112789869308\n",
            "  batch 50 loss: 0.1815614402294159\n",
            "  batch 51 loss: 0.1376543641090393\n",
            "  batch 52 loss: 0.2507089674472809\n",
            "  batch 53 loss: 0.2760334610939026\n",
            "  batch 54 loss: 0.21306918561458588\n",
            "  batch 55 loss: 0.008436489850282669\n",
            "  batch 56 loss: 0.26288214325904846\n",
            "  batch 57 loss: 0.05872340872883797\n",
            "  batch 58 loss: 0.12243470549583435\n",
            "  batch 59 loss: 0.16364067792892456\n",
            "  batch 60 loss: 0.7752355337142944\n",
            "  batch 61 loss: 0.4034920930862427\n",
            "  batch 62 loss: 0.2705744206905365\n",
            "  batch 63 loss: 0.0710325837135315\n",
            "  batch 64 loss: 0.1007259264588356\n",
            "  batch 65 loss: 0.21537545323371887\n",
            "  batch 66 loss: 0.24349822103977203\n",
            "  batch 67 loss: 0.039580587297677994\n",
            "  batch 68 loss: 0.1350332349538803\n",
            "  batch 69 loss: 0.3826424777507782\n",
            "  batch 70 loss: 0.20647577941417694\n",
            "  batch 71 loss: 0.2113482654094696\n",
            "  batch 72 loss: 0.20840473473072052\n",
            "  batch 73 loss: 0.0477147102355957\n",
            "  batch 74 loss: 0.13918475806713104\n",
            "  batch 75 loss: 0.21732324361801147\n",
            "  batch 76 loss: 0.23299479484558105\n",
            "  batch 77 loss: 0.09871995449066162\n",
            "  batch 78 loss: 0.2468937635421753\n",
            "  batch 79 loss: 0.17956209182739258\n",
            "  batch 80 loss: 0.2516081631183624\n",
            "LOSS train 0.2516081631183624 valid 0.9444608092308044\n",
            "EPOCH 20:\n",
            "  batch 1 loss: 0.04174597188830376\n",
            "  batch 2 loss: 0.21559645235538483\n",
            "  batch 3 loss: 0.09957322478294373\n",
            "  batch 4 loss: 0.10946619510650635\n",
            "  batch 5 loss: 0.14108023047447205\n",
            "  batch 6 loss: 0.19051222503185272\n",
            "  batch 7 loss: 0.08224567770957947\n",
            "  batch 8 loss: 0.09191135317087173\n",
            "  batch 9 loss: 0.03177051246166229\n",
            "  batch 10 loss: 0.08156747370958328\n",
            "  batch 11 loss: 0.20758120715618134\n",
            "  batch 12 loss: 0.8270224332809448\n",
            "  batch 13 loss: 0.1101800799369812\n",
            "  batch 14 loss: 0.006439724005758762\n",
            "  batch 15 loss: 0.020677143707871437\n",
            "  batch 16 loss: 0.32659199833869934\n",
            "  batch 17 loss: 0.2790893018245697\n",
            "  batch 18 loss: 0.23329588770866394\n",
            "  batch 19 loss: 0.31422051787376404\n",
            "  batch 20 loss: 0.11223243176937103\n",
            "  batch 21 loss: 0.21506178379058838\n",
            "  batch 22 loss: 0.18212532997131348\n",
            "  batch 23 loss: 0.16657987236976624\n",
            "  batch 24 loss: 0.2684171497821808\n",
            "  batch 25 loss: 0.4442726969718933\n",
            "  batch 26 loss: 0.057537347078323364\n",
            "  batch 27 loss: 0.07957392185926437\n",
            "  batch 28 loss: 0.17781412601470947\n",
            "  batch 29 loss: 0.16241587698459625\n",
            "  batch 30 loss: 0.1647987961769104\n",
            "  batch 31 loss: 0.06791165471076965\n",
            "  batch 32 loss: 0.1839527189731598\n",
            "  batch 33 loss: 0.08467933535575867\n",
            "  batch 34 loss: 0.2535208761692047\n",
            "  batch 35 loss: 0.23056663572788239\n",
            "  batch 36 loss: 0.11940895766019821\n",
            "  batch 37 loss: 0.2743443548679352\n",
            "  batch 38 loss: 0.658875584602356\n",
            "  batch 39 loss: 0.14189618825912476\n",
            "  batch 40 loss: 0.23740431666374207\n",
            "  batch 41 loss: 0.25184762477874756\n",
            "  batch 42 loss: 0.35488277673721313\n",
            "  batch 43 loss: 0.13083022832870483\n",
            "  batch 44 loss: 0.11320265382528305\n",
            "  batch 45 loss: 0.19545358419418335\n",
            "  batch 46 loss: 0.21670164167881012\n",
            "  batch 47 loss: 0.2634021043777466\n",
            "  batch 48 loss: 0.2070978432893753\n",
            "  batch 49 loss: 0.090038001537323\n",
            "  batch 50 loss: 0.11119799315929413\n",
            "  batch 51 loss: 0.055832043290138245\n",
            "  batch 52 loss: 0.5548311471939087\n",
            "  batch 53 loss: 0.2399706095457077\n",
            "  batch 54 loss: 0.12993691861629486\n",
            "  batch 55 loss: 0.2626783847808838\n",
            "  batch 56 loss: 0.13449987769126892\n",
            "  batch 57 loss: 0.18688847124576569\n",
            "  batch 58 loss: 0.06969266384840012\n",
            "  batch 59 loss: 0.2996668517589569\n",
            "  batch 60 loss: 0.17066343128681183\n",
            "  batch 61 loss: 0.41198429465293884\n",
            "  batch 62 loss: 0.19033512473106384\n",
            "  batch 63 loss: 0.12497613579034805\n",
            "  batch 64 loss: 0.10146337747573853\n",
            "  batch 65 loss: 0.18925653398036957\n",
            "  batch 66 loss: 0.1522853523492813\n",
            "  batch 67 loss: 0.05466018617153168\n",
            "  batch 68 loss: 0.3887765407562256\n",
            "  batch 69 loss: 0.5962592363357544\n",
            "  batch 70 loss: 0.13575688004493713\n",
            "  batch 71 loss: 0.18632569909095764\n",
            "  batch 72 loss: 0.012402521446347237\n",
            "  batch 73 loss: 0.08497478812932968\n",
            "  batch 74 loss: 0.29720258712768555\n",
            "  batch 75 loss: 0.314841091632843\n",
            "  batch 76 loss: 0.3618587255477905\n",
            "  batch 77 loss: 0.3250996470451355\n",
            "  batch 78 loss: 0.2163292020559311\n",
            "  batch 79 loss: 0.5185787081718445\n",
            "  batch 80 loss: 0.07243626564741135\n",
            "LOSS train 0.07243626564741135 valid 0.9695823788642883\n",
            "EPOCH 21:\n",
            "  batch 1 loss: 0.20164571702480316\n",
            "  batch 2 loss: 0.19560781121253967\n",
            "  batch 3 loss: 0.20820388197898865\n",
            "  batch 4 loss: 0.13207390904426575\n",
            "  batch 5 loss: 0.11245489120483398\n",
            "  batch 6 loss: 0.17799414694309235\n",
            "  batch 7 loss: 0.0819074735045433\n",
            "  batch 8 loss: 0.11812644451856613\n",
            "  batch 9 loss: 0.176715686917305\n",
            "  batch 10 loss: 0.049183931201696396\n",
            "  batch 11 loss: 0.2956368327140808\n",
            "  batch 12 loss: 0.047367483377456665\n",
            "  batch 13 loss: 0.7025141716003418\n",
            "  batch 14 loss: 0.12209071218967438\n",
            "  batch 15 loss: 0.2109176367521286\n",
            "  batch 16 loss: 0.09417650103569031\n",
            "  batch 17 loss: 0.0751967653632164\n",
            "  batch 18 loss: 0.08510847389698029\n",
            "  batch 19 loss: 0.2820874750614166\n",
            "  batch 20 loss: 0.11486056447029114\n",
            "  batch 21 loss: 0.07685869932174683\n",
            "  batch 22 loss: 0.1665966808795929\n",
            "  batch 23 loss: 0.0535888671875\n",
            "  batch 24 loss: 0.13949085772037506\n",
            "  batch 25 loss: 0.06644868850708008\n",
            "  batch 26 loss: 0.09362490475177765\n",
            "  batch 27 loss: 0.0950528085231781\n",
            "  batch 28 loss: 0.4757588505744934\n",
            "  batch 29 loss: 0.15839356184005737\n",
            "  batch 30 loss: 0.23661720752716064\n",
            "  batch 31 loss: 0.22178088128566742\n",
            "  batch 32 loss: 0.37764397263526917\n",
            "  batch 33 loss: 0.0664176344871521\n",
            "  batch 34 loss: 0.03980574384331703\n",
            "  batch 35 loss: 0.0917736142873764\n",
            "  batch 36 loss: 0.24420727789402008\n",
            "  batch 37 loss: 0.28299710154533386\n",
            "  batch 38 loss: 0.13909827172756195\n",
            "  batch 39 loss: 0.2173164039850235\n",
            "  batch 40 loss: 0.1564704179763794\n",
            "  batch 41 loss: 0.08580063283443451\n",
            "  batch 42 loss: 0.2461395263671875\n",
            "  batch 43 loss: 0.23553350567817688\n",
            "  batch 44 loss: 0.06356240063905716\n",
            "  batch 45 loss: 0.05030190944671631\n",
            "  batch 46 loss: 0.16830885410308838\n",
            "  batch 47 loss: 0.1471177041530609\n",
            "  batch 48 loss: 0.1082928478717804\n",
            "  batch 49 loss: 0.35399216413497925\n",
            "  batch 50 loss: 0.3130146265029907\n",
            "  batch 51 loss: 0.028582610189914703\n",
            "  batch 52 loss: 0.04963864013552666\n",
            "  batch 53 loss: 0.2648541033267975\n",
            "  batch 54 loss: 0.1641375571489334\n",
            "  batch 55 loss: 0.3254053592681885\n",
            "  batch 56 loss: 0.12755271792411804\n",
            "  batch 57 loss: 0.9023220539093018\n",
            "  batch 58 loss: 0.08677838742733002\n",
            "  batch 59 loss: 0.04974641650915146\n",
            "  batch 60 loss: 0.34919261932373047\n",
            "  batch 61 loss: 0.15931746363639832\n",
            "  batch 62 loss: 0.5368058085441589\n",
            "  batch 63 loss: 0.3959517776966095\n",
            "  batch 64 loss: 0.21863757073879242\n",
            "  batch 65 loss: 0.15134193003177643\n",
            "  batch 66 loss: 0.06149357184767723\n",
            "  batch 67 loss: 0.23516736924648285\n",
            "  batch 68 loss: 0.02821124717593193\n",
            "  batch 69 loss: 0.4142768383026123\n",
            "  batch 70 loss: 0.27870234847068787\n",
            "  batch 71 loss: 0.32798415422439575\n",
            "  batch 72 loss: 0.20765917003154755\n",
            "  batch 73 loss: 0.27322012186050415\n",
            "  batch 74 loss: 0.4179461598396301\n",
            "  batch 75 loss: 0.6452441215515137\n",
            "  batch 76 loss: 0.1905308961868286\n",
            "  batch 77 loss: 0.1622612327337265\n",
            "  batch 78 loss: 0.21263591945171356\n",
            "  batch 79 loss: 0.15767179429531097\n",
            "  batch 80 loss: 0.310936838388443\n",
            "LOSS train 0.310936838388443 valid 1.0044291019439697\n",
            "EPOCH 22:\n",
            "  batch 1 loss: 0.23331043124198914\n",
            "  batch 2 loss: 0.20363152027130127\n",
            "  batch 3 loss: 0.2311393767595291\n",
            "  batch 4 loss: 0.44739142060279846\n",
            "  batch 5 loss: 0.20219357311725616\n",
            "  batch 6 loss: 0.7524099946022034\n",
            "  batch 7 loss: 0.21212907135486603\n",
            "  batch 8 loss: 0.26106494665145874\n",
            "  batch 9 loss: 0.169658824801445\n",
            "  batch 10 loss: 0.14316515624523163\n",
            "  batch 11 loss: 0.11047472804784775\n",
            "  batch 12 loss: 0.14466269314289093\n",
            "  batch 13 loss: 0.33009928464889526\n",
            "  batch 14 loss: 0.2663640081882477\n",
            "  batch 15 loss: 0.11353747546672821\n",
            "  batch 16 loss: 0.1603700816631317\n",
            "  batch 17 loss: 0.3899989724159241\n",
            "  batch 18 loss: 0.30591729283332825\n",
            "  batch 19 loss: 0.1390838474035263\n",
            "  batch 20 loss: 0.13393880426883698\n",
            "  batch 21 loss: 0.4560425877571106\n",
            "  batch 22 loss: 0.3030555844306946\n",
            "  batch 23 loss: 0.20212292671203613\n",
            "  batch 24 loss: 0.11755014210939407\n",
            "  batch 25 loss: 0.22602500021457672\n",
            "  batch 26 loss: 0.26060712337493896\n",
            "  batch 27 loss: 0.3657780885696411\n",
            "  batch 28 loss: 0.18578499555587769\n",
            "  batch 29 loss: 0.08400358259677887\n",
            "  batch 30 loss: 0.1457875519990921\n",
            "  batch 31 loss: 0.05467619001865387\n",
            "  batch 32 loss: 0.10958864539861679\n",
            "  batch 33 loss: 0.23838873207569122\n",
            "  batch 34 loss: 0.045398905873298645\n",
            "  batch 35 loss: 0.07667624950408936\n",
            "  batch 36 loss: 0.014810741879045963\n",
            "  batch 37 loss: 0.1466088593006134\n",
            "  batch 38 loss: 0.0315362922847271\n",
            "  batch 39 loss: 0.13119420409202576\n",
            "  batch 40 loss: 0.17653222382068634\n",
            "  batch 41 loss: 0.38153454661369324\n",
            "  batch 42 loss: 0.06893105804920197\n",
            "  batch 43 loss: 0.07902352511882782\n",
            "  batch 44 loss: 0.17752236127853394\n",
            "  batch 45 loss: 0.19772204756736755\n",
            "  batch 46 loss: 0.642751932144165\n",
            "  batch 47 loss: 0.07188890874385834\n",
            "  batch 48 loss: 0.16638459265232086\n",
            "  batch 49 loss: 0.15324096381664276\n",
            "  batch 50 loss: 0.24766884744167328\n",
            "  batch 51 loss: 0.1675516963005066\n",
            "  batch 52 loss: 0.2744232714176178\n",
            "  batch 53 loss: 0.16630254685878754\n",
            "  batch 54 loss: 0.1363459676504135\n",
            "  batch 55 loss: 0.16907638311386108\n",
            "  batch 56 loss: 0.25444284081459045\n",
            "  batch 57 loss: 0.10894168168306351\n",
            "  batch 58 loss: 0.7289981842041016\n",
            "  batch 59 loss: 0.11982550472021103\n",
            "  batch 60 loss: 0.3475700318813324\n",
            "  batch 61 loss: 0.20441067218780518\n",
            "  batch 62 loss: 0.1279846429824829\n",
            "  batch 63 loss: 0.07907700538635254\n",
            "  batch 64 loss: 0.037034835666418076\n",
            "  batch 65 loss: 0.09744365513324738\n",
            "  batch 66 loss: 0.2127256691455841\n",
            "  batch 67 loss: 0.31532716751098633\n",
            "  batch 68 loss: 0.09592799097299576\n",
            "  batch 69 loss: 0.30477774143218994\n",
            "  batch 70 loss: 0.2031826376914978\n",
            "  batch 71 loss: 0.35796597599983215\n",
            "  batch 72 loss: 0.09039915353059769\n",
            "  batch 73 loss: 0.18603286147117615\n",
            "  batch 74 loss: 0.014246894046664238\n",
            "  batch 75 loss: 0.05214179679751396\n",
            "  batch 76 loss: 0.11097066104412079\n",
            "  batch 77 loss: 0.21120202541351318\n",
            "  batch 78 loss: 0.015112671069800854\n",
            "  batch 79 loss: 0.3861348628997803\n",
            "  batch 80 loss: 0.2788746953010559\n",
            "LOSS train 0.2788746953010559 valid 0.9629405736923218\n",
            "EPOCH 23:\n",
            "  batch 1 loss: 0.08811001479625702\n",
            "  batch 2 loss: 0.07358913868665695\n",
            "  batch 3 loss: 0.0647491067647934\n",
            "  batch 4 loss: 0.08966203778982162\n",
            "  batch 5 loss: 0.09298592805862427\n",
            "  batch 6 loss: 0.23394039273262024\n",
            "  batch 7 loss: 0.09801974892616272\n",
            "  batch 8 loss: 0.07662300765514374\n",
            "  batch 9 loss: 0.11462920904159546\n",
            "  batch 10 loss: 0.05942735821008682\n",
            "  batch 11 loss: 0.17702221870422363\n",
            "  batch 12 loss: 0.1173446848988533\n",
            "  batch 13 loss: 0.2946353554725647\n",
            "  batch 14 loss: 0.470214307308197\n",
            "  batch 15 loss: 0.161824569106102\n",
            "  batch 16 loss: 0.05042313411831856\n",
            "  batch 17 loss: 0.20048193633556366\n",
            "  batch 18 loss: 0.28894054889678955\n",
            "  batch 19 loss: 0.0830073133111\n",
            "  batch 20 loss: 0.28888291120529175\n",
            "  batch 21 loss: 0.5887954831123352\n",
            "  batch 22 loss: 0.1656196117401123\n",
            "  batch 23 loss: 0.045975781977176666\n",
            "  batch 24 loss: 0.10178066790103912\n",
            "  batch 25 loss: 0.26151567697525024\n",
            "  batch 26 loss: 0.20521637797355652\n",
            "  batch 27 loss: 0.20359250903129578\n",
            "  batch 28 loss: 0.13509097695350647\n",
            "  batch 29 loss: 0.03507740795612335\n",
            "  batch 30 loss: 0.06980102509260178\n",
            "  batch 31 loss: 0.14019885659217834\n",
            "  batch 32 loss: 0.07818274945020676\n",
            "  batch 33 loss: 0.06038833037018776\n",
            "  batch 34 loss: 0.41023609042167664\n",
            "  batch 35 loss: 0.15740114450454712\n",
            "  batch 36 loss: 0.13119906187057495\n",
            "  batch 37 loss: 0.2705298364162445\n",
            "  batch 38 loss: 0.1525806188583374\n",
            "  batch 39 loss: 0.12891805171966553\n",
            "  batch 40 loss: 0.08838152885437012\n",
            "  batch 41 loss: 0.13364195823669434\n",
            "  batch 42 loss: 0.3133608400821686\n",
            "  batch 43 loss: 0.30365028977394104\n",
            "  batch 44 loss: 0.25666433572769165\n",
            "  batch 45 loss: 0.24773210287094116\n",
            "  batch 46 loss: 0.33597588539123535\n",
            "  batch 47 loss: 0.055261388421058655\n",
            "  batch 48 loss: 0.1430787742137909\n",
            "  batch 49 loss: 0.08062171190977097\n",
            "  batch 50 loss: 0.11756838858127594\n",
            "  batch 51 loss: 0.2263517677783966\n",
            "  batch 52 loss: 0.23386628925800323\n",
            "  batch 53 loss: 0.17388704419136047\n",
            "  batch 54 loss: 0.08778508752584457\n",
            "  batch 55 loss: 0.09710059314966202\n",
            "  batch 56 loss: 0.28603532910346985\n",
            "  batch 57 loss: 0.23911398649215698\n",
            "  batch 58 loss: 0.6123401522636414\n",
            "  batch 59 loss: 0.6897846460342407\n",
            "  batch 60 loss: 0.20062020421028137\n",
            "  batch 61 loss: 0.26432135701179504\n",
            "  batch 62 loss: 0.10974929481744766\n",
            "  batch 63 loss: 0.15686076879501343\n",
            "  batch 64 loss: 0.5243770480155945\n",
            "  batch 65 loss: 0.187985360622406\n",
            "  batch 66 loss: 0.4679616689682007\n",
            "  batch 67 loss: 0.011029159650206566\n",
            "  batch 68 loss: 0.12218445539474487\n",
            "  batch 69 loss: 0.6823175549507141\n",
            "  batch 70 loss: 0.18156173825263977\n",
            "  batch 71 loss: 0.12714441120624542\n",
            "  batch 72 loss: 0.03830244392156601\n",
            "  batch 73 loss: 0.43315041065216064\n",
            "  batch 74 loss: 0.10942837595939636\n",
            "  batch 75 loss: 0.33328667283058167\n",
            "  batch 76 loss: 0.053976044058799744\n",
            "  batch 77 loss: 0.06149926036596298\n",
            "  batch 78 loss: 0.27849578857421875\n",
            "  batch 79 loss: 0.22151635587215424\n",
            "  batch 80 loss: 0.16402721405029297\n",
            "LOSS train 0.16402721405029297 valid 0.9522578120231628\n",
            "EPOCH 24:\n",
            "  batch 1 loss: 0.07464992254972458\n",
            "  batch 2 loss: 0.20990028977394104\n",
            "  batch 3 loss: 0.06870729476213455\n",
            "  batch 4 loss: 0.09004943817853928\n",
            "  batch 5 loss: 0.05139681696891785\n",
            "  batch 6 loss: 0.2129458785057068\n",
            "  batch 7 loss: 0.17092226445674896\n",
            "  batch 8 loss: 0.2785727083683014\n",
            "  batch 9 loss: 0.06004510819911957\n",
            "  batch 10 loss: 0.041532546281814575\n",
            "  batch 11 loss: 0.08301102370023727\n",
            "  batch 12 loss: 0.28418999910354614\n",
            "  batch 13 loss: 0.3341718316078186\n",
            "  batch 14 loss: 0.02762453816831112\n",
            "  batch 15 loss: 0.12221800535917282\n",
            "  batch 16 loss: 0.17055216431617737\n",
            "  batch 17 loss: 0.15978999435901642\n",
            "  batch 18 loss: 0.21467503905296326\n",
            "  batch 19 loss: 0.029775913804769516\n",
            "  batch 20 loss: 0.2141660451889038\n",
            "  batch 21 loss: 0.15904280543327332\n",
            "  batch 22 loss: 0.31262844800949097\n",
            "  batch 23 loss: 0.444537490606308\n",
            "  batch 24 loss: 0.05605703219771385\n",
            "  batch 25 loss: 0.036371663212776184\n",
            "  batch 26 loss: 0.07227610796689987\n",
            "  batch 27 loss: 0.12463735044002533\n",
            "  batch 28 loss: 0.4623255431652069\n",
            "  batch 29 loss: 0.04204404726624489\n",
            "  batch 30 loss: 0.21145975589752197\n",
            "  batch 31 loss: 0.24392688274383545\n",
            "  batch 32 loss: 0.4132924973964691\n",
            "  batch 33 loss: 0.21866843104362488\n",
            "  batch 34 loss: 0.1627413034439087\n",
            "  batch 35 loss: 0.30157825350761414\n",
            "  batch 36 loss: 0.09927337616682053\n",
            "  batch 37 loss: 0.13103513419628143\n",
            "  batch 38 loss: 0.06429624557495117\n",
            "  batch 39 loss: 0.4731959104537964\n",
            "  batch 40 loss: 0.13080757856369019\n",
            "  batch 41 loss: 0.14797496795654297\n",
            "  batch 42 loss: 0.19287386536598206\n",
            "  batch 43 loss: 0.20971445739269257\n",
            "  batch 44 loss: 0.8520223498344421\n",
            "  batch 45 loss: 0.2794058322906494\n",
            "  batch 46 loss: 0.46567875146865845\n",
            "  batch 47 loss: 0.11691339313983917\n",
            "  batch 48 loss: 0.09307053685188293\n",
            "  batch 49 loss: 0.13006219267845154\n",
            "  batch 50 loss: 0.18025599420070648\n",
            "  batch 51 loss: 0.22872719168663025\n",
            "  batch 52 loss: 0.0985611155629158\n",
            "  batch 53 loss: 0.11405559629201889\n",
            "  batch 54 loss: 0.1225326657295227\n",
            "  batch 55 loss: 0.06602804362773895\n",
            "  batch 56 loss: 0.2059406191110611\n",
            "  batch 57 loss: 0.22911766171455383\n",
            "  batch 58 loss: 0.8670989274978638\n",
            "  batch 59 loss: 0.251219242811203\n",
            "  batch 60 loss: 0.04951086640357971\n",
            "  batch 61 loss: 0.19587984681129456\n",
            "  batch 62 loss: 0.06751051545143127\n",
            "  batch 63 loss: 0.05845596268773079\n",
            "  batch 64 loss: 0.047570060938596725\n",
            "  batch 65 loss: 0.11468391120433807\n",
            "  batch 66 loss: 0.09903234243392944\n",
            "  batch 67 loss: 0.298740029335022\n",
            "  batch 68 loss: 0.4204750657081604\n",
            "  batch 69 loss: 0.12452774494886398\n",
            "  batch 70 loss: 0.34228095412254333\n",
            "  batch 71 loss: 0.08199015259742737\n",
            "  batch 72 loss: 0.167438805103302\n",
            "  batch 73 loss: 0.09293683618307114\n",
            "  batch 74 loss: 0.3820258378982544\n",
            "  batch 75 loss: 0.08823761343955994\n",
            "  batch 76 loss: 0.4721684455871582\n",
            "  batch 77 loss: 0.4470885992050171\n",
            "  batch 78 loss: 0.14681318402290344\n",
            "  batch 79 loss: 0.18423058092594147\n",
            "  batch 80 loss: 0.16695956885814667\n",
            "LOSS train 0.16695956885814667 valid 1.0626429319381714\n",
            "EPOCH 25:\n",
            "  batch 1 loss: 0.19930367171764374\n",
            "  batch 2 loss: 0.17965079843997955\n",
            "  batch 3 loss: 0.04849737882614136\n",
            "  batch 4 loss: 0.16461695730686188\n",
            "  batch 5 loss: 0.08496110141277313\n",
            "  batch 6 loss: 0.34818366169929504\n",
            "  batch 7 loss: 0.3723719120025635\n",
            "  batch 8 loss: 0.1436796635389328\n",
            "  batch 9 loss: 0.19163604080677032\n",
            "  batch 10 loss: 0.2926238179206848\n",
            "  batch 11 loss: 0.43973180651664734\n",
            "  batch 12 loss: 0.19154512882232666\n",
            "  batch 13 loss: 0.23997317254543304\n",
            "  batch 14 loss: 0.11132653057575226\n",
            "  batch 15 loss: 0.11454437673091888\n",
            "  batch 16 loss: 0.20008473098278046\n",
            "  batch 17 loss: 0.3563026189804077\n",
            "  batch 18 loss: 0.0803230032324791\n",
            "  batch 19 loss: 0.209305077791214\n",
            "  batch 20 loss: 0.23062622547149658\n",
            "  batch 21 loss: 0.15278932452201843\n",
            "  batch 22 loss: 0.3942873775959015\n",
            "  batch 23 loss: 0.09571219980716705\n",
            "  batch 24 loss: 0.0612555593252182\n",
            "  batch 25 loss: 0.7118876576423645\n",
            "  batch 26 loss: 0.30701735615730286\n",
            "  batch 27 loss: 0.02602027915418148\n",
            "  batch 28 loss: 0.18270118534564972\n",
            "  batch 29 loss: 0.2686871588230133\n",
            "  batch 30 loss: 0.057527437806129456\n",
            "  batch 31 loss: 0.016844192519783974\n",
            "  batch 32 loss: 0.21388153731822968\n",
            "  batch 33 loss: 0.024230899289250374\n",
            "  batch 34 loss: 0.23661546409130096\n",
            "  batch 35 loss: 0.47464272379875183\n",
            "  batch 36 loss: 0.06933041661977768\n",
            "  batch 37 loss: 0.0722486674785614\n",
            "  batch 38 loss: 0.06580974161624908\n",
            "  batch 39 loss: 0.13773265480995178\n",
            "  batch 40 loss: 0.16069839894771576\n",
            "  batch 41 loss: 0.1852564811706543\n",
            "  batch 42 loss: 0.23797157406806946\n",
            "  batch 43 loss: 0.1359340101480484\n",
            "  batch 44 loss: 0.028015363961458206\n",
            "  batch 45 loss: 0.12097255885601044\n",
            "  batch 46 loss: 0.23160061240196228\n",
            "  batch 47 loss: 0.22204408049583435\n",
            "  batch 48 loss: 0.13975831866264343\n",
            "  batch 49 loss: 0.39232340455055237\n",
            "  batch 50 loss: 0.536711573600769\n",
            "  batch 51 loss: 0.07304515689611435\n",
            "  batch 52 loss: 0.0967523604631424\n",
            "  batch 53 loss: 0.3433338403701782\n",
            "  batch 54 loss: 0.4314460754394531\n",
            "  batch 55 loss: 0.15616115927696228\n",
            "  batch 56 loss: 0.09411099553108215\n",
            "  batch 57 loss: 0.36373576521873474\n",
            "  batch 58 loss: 0.10807295888662338\n",
            "  batch 59 loss: 0.05628630518913269\n",
            "  batch 60 loss: 0.13364294171333313\n",
            "  batch 61 loss: 0.2032855749130249\n",
            "  batch 62 loss: 0.1469516009092331\n",
            "  batch 63 loss: 0.20621801912784576\n",
            "  batch 64 loss: 0.10557366907596588\n",
            "  batch 65 loss: 0.09307112544775009\n",
            "  batch 66 loss: 0.16945254802703857\n",
            "  batch 67 loss: 0.8437812924385071\n",
            "  batch 68 loss: 0.42137637734413147\n",
            "  batch 69 loss: 0.06969104707241058\n",
            "  batch 70 loss: 0.11579345911741257\n",
            "  batch 71 loss: 0.2924010753631592\n",
            "  batch 72 loss: 0.15697841346263885\n",
            "  batch 73 loss: 0.20252425968647003\n",
            "  batch 74 loss: 0.06327289342880249\n",
            "  batch 75 loss: 0.00499835517257452\n",
            "  batch 76 loss: 0.16109278798103333\n",
            "  batch 77 loss: 0.2417917400598526\n",
            "  batch 78 loss: 0.09649523347616196\n",
            "  batch 79 loss: 0.3829541504383087\n",
            "  batch 80 loss: 0.12927298247814178\n",
            "LOSS train 0.12927298247814178 valid 1.0201661586761475\n",
            "EPOCH 26:\n",
            "  batch 1 loss: 0.1800519824028015\n",
            "  batch 2 loss: 0.1926366239786148\n",
            "  batch 3 loss: 0.8140621781349182\n",
            "  batch 4 loss: 0.062464505434036255\n",
            "  batch 5 loss: 0.1333206295967102\n",
            "  batch 6 loss: 0.1256723254919052\n",
            "  batch 7 loss: 0.26644259691238403\n",
            "  batch 8 loss: 0.1992833912372589\n",
            "  batch 9 loss: 0.1994505375623703\n",
            "  batch 10 loss: 0.27484241127967834\n",
            "  batch 11 loss: 0.1768665909767151\n",
            "  batch 12 loss: 0.05775001645088196\n",
            "  batch 13 loss: 0.12045761197805405\n",
            "  batch 14 loss: 0.0971594750881195\n",
            "  batch 15 loss: 0.12887446582317352\n",
            "  batch 16 loss: 0.06132912635803223\n",
            "  batch 17 loss: 0.07609998434782028\n",
            "  batch 18 loss: 0.310909628868103\n",
            "  batch 19 loss: 0.142281636595726\n",
            "  batch 20 loss: 0.04179512709379196\n",
            "  batch 21 loss: 0.15524303913116455\n",
            "  batch 22 loss: 0.5821576118469238\n",
            "  batch 23 loss: 0.33567696809768677\n",
            "  batch 24 loss: 0.16063541173934937\n",
            "  batch 25 loss: 0.23938721418380737\n",
            "  batch 26 loss: 0.022469885647296906\n",
            "  batch 27 loss: 0.15991032123565674\n",
            "  batch 28 loss: 0.32695040106773376\n",
            "  batch 29 loss: 0.5218096971511841\n",
            "  batch 30 loss: 0.11856597661972046\n",
            "  batch 31 loss: 0.25603970885276794\n",
            "  batch 32 loss: 0.16579405963420868\n",
            "  batch 33 loss: 0.22723698616027832\n",
            "  batch 34 loss: 0.27260035276412964\n",
            "  batch 35 loss: 0.18330270051956177\n",
            "  batch 36 loss: 0.14395283162593842\n",
            "  batch 37 loss: 0.11857642978429794\n",
            "  batch 38 loss: 0.040111713111400604\n",
            "  batch 39 loss: 0.1673504263162613\n",
            "  batch 40 loss: 0.0598612055182457\n",
            "  batch 41 loss: 0.2242608517408371\n",
            "  batch 42 loss: 0.1816997528076172\n",
            "  batch 43 loss: 0.44782865047454834\n",
            "  batch 44 loss: 0.23652346432209015\n",
            "  batch 45 loss: 0.06466583162546158\n",
            "  batch 46 loss: 0.41485828161239624\n",
            "  batch 47 loss: 0.3991889953613281\n",
            "  batch 48 loss: 0.10168399661779404\n",
            "  batch 49 loss: 0.21480198204517365\n",
            "  batch 50 loss: 0.08513329178094864\n",
            "  batch 51 loss: 0.5414882302284241\n",
            "  batch 52 loss: 0.18850567936897278\n",
            "  batch 53 loss: 0.2639429271221161\n",
            "  batch 54 loss: 0.21382689476013184\n",
            "  batch 55 loss: 0.43612539768218994\n",
            "  batch 56 loss: 0.012983659282326698\n",
            "  batch 57 loss: 0.2320886254310608\n",
            "  batch 58 loss: 0.12946316599845886\n",
            "  batch 59 loss: 0.18285633623600006\n",
            "  batch 60 loss: 0.15598100423812866\n",
            "  batch 61 loss: 0.07327501475811005\n",
            "  batch 62 loss: 0.11502894759178162\n",
            "  batch 63 loss: 0.22301390767097473\n",
            "  batch 64 loss: 0.0845770612359047\n",
            "  batch 65 loss: 0.16825902462005615\n",
            "  batch 66 loss: 0.13114556670188904\n",
            "  batch 67 loss: 0.1229911595582962\n",
            "  batch 68 loss: 0.1628478318452835\n",
            "  batch 69 loss: 0.2688748836517334\n",
            "  batch 70 loss: 0.047637805342674255\n",
            "  batch 71 loss: 0.34471020102500916\n",
            "  batch 72 loss: 0.16585585474967957\n",
            "  batch 73 loss: 0.05230460315942764\n",
            "  batch 74 loss: 0.19937759637832642\n",
            "  batch 75 loss: 0.3700413703918457\n",
            "  batch 76 loss: 0.4960179924964905\n",
            "  batch 77 loss: 0.22601041197776794\n",
            "  batch 78 loss: 0.16958209872245789\n",
            "  batch 79 loss: 0.023092515766620636\n",
            "  batch 80 loss: 0.0036225777585059404\n",
            "LOSS train 0.0036225777585059404 valid 0.9769688844680786\n",
            "EPOCH 27:\n",
            "  batch 1 loss: 0.0837961882352829\n",
            "  batch 2 loss: 0.21440179646015167\n",
            "  batch 3 loss: 0.06724964827299118\n",
            "  batch 4 loss: 0.29722389578819275\n",
            "  batch 5 loss: 0.2866736948490143\n",
            "  batch 6 loss: 0.13587793707847595\n",
            "  batch 7 loss: 0.08901304751634598\n",
            "  batch 8 loss: 0.16246721148490906\n",
            "  batch 9 loss: 0.6456655263900757\n",
            "  batch 10 loss: 0.10449202358722687\n",
            "  batch 11 loss: 0.2605281174182892\n",
            "  batch 12 loss: 0.12407375127077103\n",
            "  batch 13 loss: 0.39873331785202026\n",
            "  batch 14 loss: 0.19286853075027466\n",
            "  batch 15 loss: 0.12204816937446594\n",
            "  batch 16 loss: 0.2581947147846222\n",
            "  batch 17 loss: 0.30609115958213806\n",
            "  batch 18 loss: 0.17830848693847656\n",
            "  batch 19 loss: 0.18958264589309692\n",
            "  batch 20 loss: 0.03195347264409065\n",
            "  batch 21 loss: 0.08125138282775879\n",
            "  batch 22 loss: 0.34159767627716064\n",
            "  batch 23 loss: 0.2470027506351471\n",
            "  batch 24 loss: 0.06174062564969063\n",
            "  batch 25 loss: 0.059480227530002594\n",
            "  batch 26 loss: 0.05369944125413895\n",
            "  batch 27 loss: 0.13070151209831238\n",
            "  batch 28 loss: 0.4267410933971405\n",
            "  batch 29 loss: 0.21243882179260254\n",
            "  batch 30 loss: 0.05903254821896553\n",
            "  batch 31 loss: 0.23428457975387573\n",
            "  batch 32 loss: 0.08921940624713898\n",
            "  batch 33 loss: 1.031436800956726\n",
            "  batch 34 loss: 0.10908936709165573\n",
            "  batch 35 loss: 0.059528615325689316\n",
            "  batch 36 loss: 0.1426192820072174\n",
            "  batch 37 loss: 0.34677696228027344\n",
            "  batch 38 loss: 0.14849703013896942\n",
            "  batch 39 loss: 0.2857239246368408\n",
            "  batch 40 loss: 0.24634724855422974\n",
            "  batch 41 loss: 0.6563631892204285\n",
            "  batch 42 loss: 0.007385557983070612\n",
            "  batch 43 loss: 0.25297752022743225\n",
            "  batch 44 loss: 0.09634501487016678\n",
            "  batch 45 loss: 0.14318859577178955\n",
            "  batch 46 loss: 0.30131664872169495\n",
            "  batch 47 loss: 0.1344233602285385\n",
            "  batch 48 loss: 0.0904483050107956\n",
            "  batch 49 loss: 0.2782152593135834\n",
            "  batch 50 loss: 0.09242486208677292\n",
            "  batch 51 loss: 0.2358151078224182\n",
            "  batch 52 loss: 0.18929532170295715\n",
            "  batch 53 loss: 0.1076665073633194\n",
            "  batch 54 loss: 0.10098879784345627\n",
            "  batch 55 loss: 0.11766362190246582\n",
            "  batch 56 loss: 0.08429557830095291\n",
            "  batch 57 loss: 0.5520797371864319\n",
            "  batch 58 loss: 0.1626746952533722\n",
            "  batch 59 loss: 0.16512998938560486\n",
            "  batch 60 loss: 0.3604842722415924\n",
            "  batch 61 loss: 0.00901101902127266\n",
            "  batch 62 loss: 0.15946373343467712\n",
            "  batch 63 loss: 0.053403340280056\n",
            "  batch 64 loss: 0.2718157172203064\n",
            "  batch 65 loss: 0.14046530425548553\n",
            "  batch 66 loss: 0.01478152722120285\n",
            "  batch 67 loss: 0.5941706895828247\n",
            "  batch 68 loss: 0.04474704712629318\n",
            "  batch 69 loss: 0.1368183195590973\n",
            "  batch 70 loss: 0.06263075768947601\n",
            "  batch 71 loss: 0.10255222022533417\n",
            "  batch 72 loss: 0.1998041719198227\n",
            "  batch 73 loss: 0.3824997544288635\n",
            "  batch 74 loss: 0.14367811381816864\n",
            "  batch 75 loss: 0.18951666355133057\n",
            "  batch 76 loss: 0.09555208683013916\n",
            "  batch 77 loss: 0.1675727665424347\n",
            "  batch 78 loss: 0.12587998807430267\n",
            "  batch 79 loss: 0.22210198640823364\n",
            "  batch 80 loss: 0.38707202672958374\n",
            "LOSS train 0.38707202672958374 valid 0.9606236219406128\n",
            "EPOCH 28:\n",
            "  batch 1 loss: 0.10812855511903763\n",
            "  batch 2 loss: 0.8500539660453796\n",
            "  batch 3 loss: 0.03051333874464035\n",
            "  batch 4 loss: 0.036757707595825195\n",
            "  batch 5 loss: 0.164602130651474\n",
            "  batch 6 loss: 0.14625300467014313\n",
            "  batch 7 loss: 0.30879172682762146\n",
            "  batch 8 loss: 0.1463937759399414\n",
            "  batch 9 loss: 0.41715914011001587\n",
            "  batch 10 loss: 0.30159956216812134\n",
            "  batch 11 loss: 0.08627386391162872\n",
            "  batch 12 loss: 0.4750562012195587\n",
            "  batch 13 loss: 0.06354831904172897\n",
            "  batch 14 loss: 0.23678064346313477\n",
            "  batch 15 loss: 0.08850641548633575\n",
            "  batch 16 loss: 0.16280055046081543\n",
            "  batch 17 loss: 0.08920016884803772\n",
            "  batch 18 loss: 0.08789819478988647\n",
            "  batch 19 loss: 0.11491923034191132\n",
            "  batch 20 loss: 0.34123164415359497\n",
            "  batch 21 loss: 0.24497637152671814\n",
            "  batch 22 loss: 0.130280002951622\n",
            "  batch 23 loss: 0.4473915994167328\n",
            "  batch 24 loss: 0.46930065751075745\n",
            "  batch 25 loss: 0.20374299585819244\n",
            "  batch 26 loss: 0.17820465564727783\n",
            "  batch 27 loss: 0.1567518413066864\n",
            "  batch 28 loss: 0.2707867920398712\n",
            "  batch 29 loss: 0.01260619331151247\n",
            "  batch 30 loss: 0.03807421773672104\n",
            "  batch 31 loss: 0.0766349583864212\n",
            "  batch 32 loss: 0.3755744695663452\n",
            "  batch 33 loss: 0.22751277685165405\n",
            "  batch 34 loss: 0.1480085402727127\n",
            "  batch 35 loss: 0.0843251496553421\n",
            "  batch 36 loss: 0.08525802940130234\n",
            "  batch 37 loss: 0.1123068779706955\n",
            "  batch 38 loss: 0.32362690567970276\n",
            "  batch 39 loss: 0.5386820435523987\n",
            "  batch 40 loss: 0.18434235453605652\n",
            "  batch 41 loss: 0.08275116235017776\n",
            "  batch 42 loss: 0.18007704615592957\n",
            "  batch 43 loss: 0.282630980014801\n",
            "  batch 44 loss: 0.11299024522304535\n",
            "  batch 45 loss: 0.35340267419815063\n",
            "  batch 46 loss: 0.16092222929000854\n",
            "  batch 47 loss: 0.2382241189479828\n",
            "  batch 48 loss: 0.1119164526462555\n",
            "  batch 49 loss: 0.025320567190647125\n",
            "  batch 50 loss: 0.1202603206038475\n",
            "  batch 51 loss: 0.10573171079158783\n",
            "  batch 52 loss: 0.37169983983039856\n",
            "  batch 53 loss: 0.09415953606367111\n",
            "  batch 54 loss: 0.16370080411434174\n",
            "  batch 55 loss: 0.20506861805915833\n",
            "  batch 56 loss: 0.08062904328107834\n",
            "  batch 57 loss: 0.24130457639694214\n",
            "  batch 58 loss: 0.021292081102728844\n",
            "  batch 59 loss: 0.37105345726013184\n",
            "  batch 60 loss: 0.0825386792421341\n",
            "  batch 61 loss: 0.07254262268543243\n",
            "  batch 62 loss: 0.046290360391139984\n",
            "  batch 63 loss: 0.20624063909053802\n",
            "  batch 64 loss: 0.059367697685956955\n",
            "  batch 65 loss: 0.11884107440710068\n",
            "  batch 66 loss: 0.08725252002477646\n",
            "  batch 67 loss: 0.20581449568271637\n",
            "  batch 68 loss: 0.21179942786693573\n",
            "  batch 69 loss: 0.1090439110994339\n",
            "  batch 70 loss: 0.36662498116493225\n",
            "  batch 71 loss: 0.07929911464452744\n",
            "  batch 72 loss: 0.16662545502185822\n",
            "  batch 73 loss: 0.10204708576202393\n",
            "  batch 74 loss: 0.09162598848342896\n",
            "  batch 75 loss: 0.49757900834083557\n",
            "  batch 76 loss: 0.4601767659187317\n",
            "  batch 77 loss: 0.5490203499794006\n",
            "  batch 78 loss: 0.29923051595687866\n",
            "  batch 79 loss: 0.19098113477230072\n",
            "  batch 80 loss: 0.30468669533729553\n",
            "LOSS train 0.30468669533729553 valid 0.9397351145744324\n",
            "EPOCH 29:\n",
            "  batch 1 loss: 0.1100960373878479\n",
            "  batch 2 loss: 0.1425568163394928\n",
            "  batch 3 loss: 0.20083840191364288\n",
            "  batch 4 loss: 0.2194283902645111\n",
            "  batch 5 loss: 0.28196945786476135\n",
            "  batch 6 loss: 0.05157197266817093\n",
            "  batch 7 loss: 0.27482014894485474\n",
            "  batch 8 loss: 0.11381898075342178\n",
            "  batch 9 loss: 0.1049223318696022\n",
            "  batch 10 loss: 0.37883979082107544\n",
            "  batch 11 loss: 0.0898471400141716\n",
            "  batch 12 loss: 0.41915011405944824\n",
            "  batch 13 loss: 0.04551008716225624\n",
            "  batch 14 loss: 0.1837277114391327\n",
            "  batch 15 loss: 0.0521937720477581\n",
            "  batch 16 loss: 0.3625756502151489\n",
            "  batch 17 loss: 0.17317886650562286\n",
            "  batch 18 loss: 0.33040934801101685\n",
            "  batch 19 loss: 0.22565291821956635\n",
            "  batch 20 loss: 0.41552698612213135\n",
            "  batch 21 loss: 0.14485400915145874\n",
            "  batch 22 loss: 0.333404541015625\n",
            "  batch 23 loss: 0.1249554380774498\n",
            "  batch 24 loss: 0.520432710647583\n",
            "  batch 25 loss: 0.5427666902542114\n",
            "  batch 26 loss: 0.19790363311767578\n",
            "  batch 27 loss: 0.16018548607826233\n",
            "  batch 28 loss: 0.15490077435970306\n",
            "  batch 29 loss: 0.46835988759994507\n",
            "  batch 30 loss: 0.16289860010147095\n",
            "  batch 31 loss: 0.14053305983543396\n",
            "  batch 32 loss: 0.43212053179740906\n",
            "  batch 33 loss: 0.30946284532546997\n",
            "  batch 34 loss: 0.21514534950256348\n",
            "  batch 35 loss: 0.25469183921813965\n",
            "  batch 36 loss: 0.1603548377752304\n",
            "  batch 37 loss: 0.13276062905788422\n",
            "  batch 38 loss: 0.08917565643787384\n",
            "  batch 39 loss: 0.14343933761119843\n",
            "  batch 40 loss: 0.07546158134937286\n",
            "  batch 41 loss: 0.19331848621368408\n",
            "  batch 42 loss: 0.08504142612218857\n",
            "  batch 43 loss: 0.5439653992652893\n",
            "  batch 44 loss: 0.03998446464538574\n",
            "  batch 45 loss: 0.3147386908531189\n",
            "  batch 46 loss: 0.028069086372852325\n",
            "  batch 47 loss: 0.18132534623146057\n",
            "  batch 48 loss: 0.23536162078380585\n",
            "  batch 49 loss: 0.024304641410708427\n",
            "  batch 50 loss: 0.1295972317457199\n",
            "  batch 51 loss: 0.10841801762580872\n",
            "  batch 52 loss: 0.08439794182777405\n",
            "  batch 53 loss: 0.32502269744873047\n",
            "  batch 54 loss: 0.09043064713478088\n",
            "  batch 55 loss: 0.22418898344039917\n",
            "  batch 56 loss: 0.2192845195531845\n",
            "  batch 57 loss: 0.10061526298522949\n",
            "  batch 58 loss: 0.05895014852285385\n",
            "  batch 59 loss: 0.06119154393672943\n",
            "  batch 60 loss: 0.20315459370613098\n",
            "  batch 61 loss: 0.06106923520565033\n",
            "  batch 62 loss: 0.22149795293807983\n",
            "  batch 63 loss: 0.2766615152359009\n",
            "  batch 64 loss: 0.7102969884872437\n",
            "  batch 65 loss: 0.08102831244468689\n",
            "  batch 66 loss: 0.13827018439769745\n",
            "  batch 67 loss: 0.17964357137680054\n",
            "  batch 68 loss: 0.08870786428451538\n",
            "  batch 69 loss: 0.0865410566329956\n",
            "  batch 70 loss: 0.2743286192417145\n",
            "  batch 71 loss: 0.17451587319374084\n",
            "  batch 72 loss: 0.18814720213413239\n",
            "  batch 73 loss: 0.19123302400112152\n",
            "  batch 74 loss: 0.14763079583644867\n",
            "  batch 75 loss: 0.1877773404121399\n",
            "  batch 76 loss: 0.10989326983690262\n",
            "  batch 77 loss: 0.10484902560710907\n",
            "  batch 78 loss: 0.21192054450511932\n",
            "  batch 79 loss: 0.06454453617334366\n",
            "  batch 80 loss: 0.011510876007378101\n",
            "LOSS train 0.011510876007378101 valid 0.9053903222084045\n",
            "EPOCH 30:\n",
            "  batch 1 loss: 0.14007337391376495\n",
            "  batch 2 loss: 0.12913145124912262\n",
            "  batch 3 loss: 0.1844683438539505\n",
            "  batch 4 loss: 0.14729954302310944\n",
            "  batch 5 loss: 0.31736305356025696\n",
            "  batch 6 loss: 0.41347429156303406\n",
            "  batch 7 loss: 0.1322982758283615\n",
            "  batch 8 loss: 0.5597394704818726\n",
            "  batch 9 loss: 0.08688993752002716\n",
            "  batch 10 loss: 0.014294873923063278\n",
            "  batch 11 loss: 0.21437227725982666\n",
            "  batch 12 loss: 0.0625264123082161\n",
            "  batch 13 loss: 0.28864210844039917\n",
            "  batch 14 loss: 0.08312558382749557\n",
            "  batch 15 loss: 0.24099820852279663\n",
            "  batch 16 loss: 0.11055444180965424\n",
            "  batch 17 loss: 0.4846724271774292\n",
            "  batch 18 loss: 0.32793450355529785\n",
            "  batch 19 loss: 0.23165923357009888\n",
            "  batch 20 loss: 0.012861687690019608\n",
            "  batch 21 loss: 0.2326035499572754\n",
            "  batch 22 loss: 0.07746376097202301\n",
            "  batch 23 loss: 0.2644784450531006\n",
            "  batch 24 loss: 0.1500011831521988\n",
            "  batch 25 loss: 0.11245281249284744\n",
            "  batch 26 loss: 0.17575109004974365\n",
            "  batch 27 loss: 0.025263700634241104\n",
            "  batch 28 loss: 0.02558562159538269\n",
            "  batch 29 loss: 0.15110071003437042\n",
            "  batch 30 loss: 0.024513177573680878\n",
            "  batch 31 loss: 0.09191346913576126\n",
            "  batch 32 loss: 0.1817740797996521\n",
            "  batch 33 loss: 0.1752992570400238\n",
            "  batch 34 loss: 0.2984856367111206\n",
            "  batch 35 loss: 0.14102111756801605\n",
            "  batch 36 loss: 0.10672172904014587\n",
            "  batch 37 loss: 0.16687943041324615\n",
            "  batch 38 loss: 0.39361652731895447\n",
            "  batch 39 loss: 0.08980420976877213\n",
            "  batch 40 loss: 0.21511918306350708\n",
            "  batch 41 loss: 0.2052260935306549\n",
            "  batch 42 loss: 0.23190441727638245\n",
            "  batch 43 loss: 0.1214178055524826\n",
            "  batch 44 loss: 0.5520594716072083\n",
            "  batch 45 loss: 0.7492817640304565\n",
            "  batch 46 loss: 0.058799900114536285\n",
            "  batch 47 loss: 0.024466445669531822\n",
            "  batch 48 loss: 0.34358444809913635\n",
            "  batch 49 loss: 0.23825395107269287\n",
            "  batch 50 loss: 0.12335851788520813\n",
            "  batch 51 loss: 0.07209891080856323\n",
            "  batch 52 loss: 0.059867922216653824\n",
            "  batch 53 loss: 0.5434006452560425\n",
            "  batch 54 loss: 0.2904612720012665\n",
            "  batch 55 loss: 0.09526536613702774\n",
            "  batch 56 loss: 0.44156599044799805\n",
            "  batch 57 loss: 0.13286332786083221\n",
            "  batch 58 loss: 0.06345658004283905\n",
            "  batch 59 loss: 0.3027648329734802\n",
            "  batch 60 loss: 0.08106229454278946\n",
            "  batch 61 loss: 0.10947900265455246\n",
            "  batch 62 loss: 0.6137768030166626\n",
            "  batch 63 loss: 0.3259812891483307\n",
            "  batch 64 loss: 0.18731005489826202\n",
            "  batch 65 loss: 0.26515111327171326\n",
            "  batch 66 loss: 0.4979301393032074\n",
            "  batch 67 loss: 0.19502118229866028\n",
            "  batch 68 loss: 0.13566049933433533\n",
            "  batch 69 loss: 0.1006227657198906\n",
            "  batch 70 loss: 0.2287200540304184\n",
            "  batch 71 loss: 0.10644428431987762\n",
            "  batch 72 loss: 0.24192394316196442\n",
            "  batch 73 loss: 0.2833661735057831\n",
            "  batch 74 loss: 0.2908758223056793\n",
            "  batch 75 loss: 0.1566721647977829\n",
            "  batch 76 loss: 0.014721735380589962\n",
            "  batch 77 loss: 0.09990332275629044\n",
            "  batch 78 loss: 0.11896181106567383\n",
            "  batch 79 loss: 0.08283314853906631\n",
            "  batch 80 loss: 0.1306992769241333\n",
            "LOSS train 0.1306992769241333 valid 0.9402636289596558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **III. Test Model with particular Input constructing**"
      ],
      "metadata": {
        "id": "ND0S_Ktrp0H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Test Sample data\n",
        "def testDatapoint(lat, lon, year, month, width = 1000):\n",
        "  # One datapoint\n",
        "  # inject rawPm25\n",
        "  X = getRawPM25(lat, lon, year, month, width).reshape(1, -1)\n",
        "  # inject traffic tile\n",
        "  X = np.concatenate((X, getTrafficTile(lat, lon, width).reshape(1, -1)), axis = 1)\n",
        "  # inject land cover\n",
        "  X = np.concatenate((X, getLandCover(lat, lon, width).reshape(1, -1)), axis = 1)\n",
        "  # inject road density\n",
        "  X = np.concatenate((X, getRoadDensity(lat, lon, width).reshape(1, -1)), axis = 1)\n",
        "  # inject population density\n",
        "  X = np.concatenate((X, getPopDensity(lat, lon, year, width).reshape(1, -1)), axis = 1)\n",
        "  # inject VVNB feature\n",
        "  X = np.concatenate((X, getVVNB(year, month).reshape(1, -1)), axis = 1)\n",
        "  return X.reshape(19)\n",
        "\n",
        "def getTruePM25(lat, lon, year, month):\n",
        "  strquery = \"Year == \" + str(year) + \" and Month == \" + str(month)\n",
        "  strquery = strquery + \" and lat == \" + str(lat) + \" and lon == \" + str(lon)\n",
        "  print(Month_Series_AVE.query(strquery))\n",
        "  return Month_Series_AVE.query(strquery).iloc[0, 0]\n"
      ],
      "metadata": {
        "id": "0i6G_ENWuZRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = 6\n",
        "print(Month_Series_AVE.iloc[num, 1], Month_Series_AVE.iloc[num, 2], Month_Series_AVE.iloc[num, 3], Month_Series_AVE.iloc[num, 4])\n",
        "with torch.no_grad():\n",
        "    x_in = []\n",
        "    x_in.append(testDatapoint(Month_Series_AVE.iloc[num, 1], Month_Series_AVE.iloc[num, 2], Month_Series_AVE.iloc[num, 3], Month_Series_AVE.iloc[num, 4]))\n",
        "    x = torch.Tensor(x_in)\n",
        "    x.unsqueeze(1)\n",
        "    ypred = model((x - tensor_x_mean)/tensor_x_std)\n",
        "    # unnormlalize prediction\n",
        "    ypred = ypred * tensor_y_std\n",
        "    ypred = ypred + tensor_y_mean"
      ],
      "metadata": {
        "id": "6Smcr054pX8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa7007f-82ce-4b31-cc35-d747d3484857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.002399999999778 105.71809999999863 2020 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ypred.item())\n",
        "print(getTruePM25(Month_Series_AVE.iloc[num, 1], Month_Series_AVE.iloc[num, 2], Month_Series_AVE.iloc[num, 3], Month_Series_AVE.iloc[num, 4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3EEveNMxdzl",
        "outputId": "e8fec49b-f9ce-4060-de62-a3cef6d659c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23.054718017578125\n",
            "                         pm25      lat       lon  Year  Month\n",
            "name    Year-Month                                           \n",
            "ankhanh 2020-8      12.711051  21.0024  105.7181  2020      8\n",
            "12.711051212938006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = GarmentClassifier()\n",
        "saved_model.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "ot9SnUa_f3GB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}